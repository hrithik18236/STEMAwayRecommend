{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vg1lWugC3gWZ"
   },
   "source": [
    "# 1. Exploratory Data Analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFagFrw-gj39"
   },
   "source": [
    "## 1.1 Let's load our data\n",
    "\n",
    "I have loaded from Google sheets below as the raw CSVs needed a little extra cleaning, so I did this in Sheets vs loading the CSVs and then cleaning in Pandas (just because it was quicker)! :) \n",
    "\n",
    "However, please feel free to experiment loading your data in a different way! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8476,
     "status": "ok",
     "timestamp": 1596991111393,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "z12OxhRJfZcR",
    "outputId": "db6247bc-5507-4d38-eacd-2a56097a5ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
      "Requirement already satisfied: gspread in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
      "Requirement already satisfied: gspread-dataframe in /usr/local/lib/python3.6/dist-packages (3.0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 11.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Collecting sacremoses\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c43a13515788e2cedc79c175d2298e806cf744f59c74c3721fb5a9137e52e546\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch pandas gspread gspread-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17265,
     "status": "ok",
     "timestamp": 1596991131993,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "1FHEeL5TgCD7",
    "outputId": "8eb9cbe2-4931-436d-ec96-f5e30397d49a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "default_app = GoogleCredentials.get_application_default()\n",
    "gc = gspread.authorize(default_app)\n",
    "# Display full col_width\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "SPREADSHEET_NAMES = ['huel_forum_posts', \n",
    "                     'hopscotch_forum_posts',\n",
    "                     'bnz_forum_posts', \n",
    "                     'schizophrenia_forum_posts', \n",
    "                     'airline_forum_posts',\n",
    "                     'quickfile_forum_posts']\n",
    "\n",
    "def loadgsheet(spreadsheet_name):\n",
    "  spreadsheet = gc.open(spreadsheet_name)\n",
    "  worksheet = spreadsheet.worksheet(title=spreadsheet_name)\n",
    "  columns=['post_text', 'post_id', 'user_id',\t'username',\t'topic_id',\t'post_num',\t'reply_count',\t'created_at',\t'updated_at',\t'num_reads',\t'topic_slug',\t'forum_name']\n",
    "  return get_as_dataframe(worksheet)[:1000][columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50381,
     "status": "ok",
     "timestamp": 1596991187291,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "GUyBVAypgJ21",
    "outputId": "31076a1e-5151-4a5d-cd23-47996f788e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining huel_forum_posts\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       post_text  ...  forum_name\n",
      "0  Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)   ...  huel      \n",
      "1  i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  huel      \n",
      "\n",
      "[2 rows x 12 columns]\n",
      "Joining hopscotch_forum_posts\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               post_text  ... forum_name\n",
      "0  Anything about the Hopscotch WebPlayer? Talk about that here! creationsofanoob\">CreationsOfaNoob\\nThis can include possibly modding the webplayer or a certain part of how it works in general.\\nI made a small tweak to “fix” line spacing issues with the “copied glitches” project, but I know very little about how the whole WebPlayer works.\\nTell me if I am wrong, but I think I am the first here to mod the WebPlayer.\\nHow does it relate to HS?\\nWe can see how the webplayer can improve by analyzing and tweaking small parts about it.  ...  hopscotch\n",
      "1  This is a great idea! My iPad is a “family iPad” so I don’t want to change any code and JSONs because of that and also because that I don’t want my Hopscotch App to stop working. But I am interested in how the web player works? Does it have to be modified for each project or can you just modify a single file or directory? And how does these changes take effect? Do you just have to publish a project with a modified web player for it to work?                                                                                           ...  hopscotch\n",
      "\n",
      "[2 rows x 12 columns]\n",
      "Joining bnz_forum_posts\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                post_text  ...  forum_name\n",
      "0  Today is **‘Safer Internet Day’** , which is celebrated globally in February each year. It’s a day dedicated to promoting a safer online world. Netsafe are the official organising committee for this in New Zealand, and BNZ are proud to support them to encourage conversations about what a safer internet could look like. At BNZ, we think it’s an important day to recognise given that if you’re not staying safe online, you’re more likely to become a target of financial crime.     Netsafe have shared some great tips and resources, including online safety conversation starters for kids (under ten year olds), tweens (10-13 year olds) and teens (14-18 year olds). They recommend having regular chats with your young ones to educate them about online safety, and ultimately help minimise the damage if things do go wrong online. The types of questions they suggest asking your children are:  * What are some things you shouldn’t share online?  * What is the difference between public and private posts?  * Do you know what privacy settings are and what they are used for?    On top of this, Netsafe have a ‘Parents Toolkit’, as well as guides for checking privacy settings for some of the major social media channels – Facebook, Twitter, Instagram and Snapchat.     To read these resources in detail, and even print them off to take home so you can ask your kids, visit the following page <https://www.netsafe.org.nz/safer- internet-day/>     [![FB & IG Proudly supporting 2](https://aws1.discourse- cdn.com/bnz/optimized/2X/d/daa286b1727bf23324a80964f2bcf67079936775_2_500x500.jpeg) FB &amp; IG Proudly supporting 21200×1200 519 KB ](https://aws1.discourse- cdn.com/bnz/original/2X/d/daa286b1727bf23324a80964f2bcf67079936775.jpeg \"FB &amp; IG Proudly supporting 2\")   ...  bnz       \n",
      "1  When I go online to set a end date on a automatic payment it will not allow a date to be entered forcing me to call to get it done.  I was told it’s a known issue but is there any ideas on when the issue will be fixed, it’s been months and months since the new internet banking was rolled out and it’s never worked on this system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  bnz       \n",
      "\n",
      "[2 rows x 12 columns]\n",
      "Joining schizophrenia_forum_posts\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             post_text  ...     forum_name\n",
      "0  ![](http://www.peteearley.com/wp-content/themes/education/images/favicon.ico) [Pete Earley – 8 Jun 20](http://www.peteearley.com/2020/06/08/federal-govt- accused-of-abandoning-research-that-would-provide-short-term-help-to-the-most- seriously-mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29 \"12:00PM - 08 June 2020\") ![](https://aws1.discourse- cdn.com/schizophrenia/uploads/default/original/3X/f/2/f2e8cca16cf15cebbe4ee807f64c29a9dcc09d25.jpeg) ### [Federal Govt. Accused Of Abandoning Research That Would Provide Short Term...](http://www.peteearley.com/2020/06/08/federal-govt-accused-of- abandoning-research-that-would-provide-short-term-help-to-the-most-seriously- mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29) Dr. E. Fuller Torrey rips into NIMH, its advisory board and NAMI (6-8-20) Dr. E. Fuller Torrey is again accusing the National Institutes of Mental Health of virtually abandoning clinical trials that could help Americans with schizophrenia and bipolar...   ...  schizophrenia\n",
      "1  I’m taking my first capsule of Lumateperone 42 mg tonight. Steady state should be reached in 5 days. Will share my experiences here.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  schizophrenia\n",
      "\n",
      "[2 rows x 12 columns]\n",
      "Joining airline_forum_posts\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   post_text  ...  forum_name\n",
      "0  Hey all - April credited 96 trips (~ 84 hours) of pay with 21 days off. Definitely much improved on loads this month, seeing quite a few full to the new limited capacity (~67%). It was rare to see less than 40 people per flight by the end of the month and noticed a LOT more people in the airport terminals. Hoping to see this trend continue!  MCO-HOU (DH) HOU-MAF-LAS-PHX-MCO (1st leg DH) OFF OFF OFF OFF OFF OFF OFF OFF OFF OFF OFF MCO-DEN-STL STL-BWI-ISP-BWI-JAX JAX-HOU-ATL-GSP GSP-ATL-MDW-MCO OFF OFF OFF OFF OFF OFF OFF MCO-DEN-STL STL-BWI-ISP-BWI-JAX JAX-HOU-ATL-GSP GSP-ATL-MDW-MCO OFF OFF OFF  ...  airline   \n",
      "1  It’s great to see a rebound, even if it’s small. Have you heard of any plans for Southwest to expand routes? I would think that with some of the legacies plan to reduce their routes, it might be a good time for airlines like LUv and JBLU to pick up market share strategically (without taking on too much additional variable costs).                                                                                                                                                                                                                                                                                ...  airline   \n",
      "\n",
      "[2 rows x 12 columns]\n",
      "Joining quickfile_forum_posts\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          post_text  ...  forum_name\n",
      "0  HI, Just disconnected from Santander to change to Starling and it isn’t listed?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  quickfile \n",
      "1  Hi [@daveh001](/u/daveh001) Are you trying to connect this from “Open Banking” by any chance? The Starling feed isn’t actually Open Banking (although I believe this will change in the future). As long as you see the Starling logo on your account in QuickFile, you can go to the bank statement view, select **More Options** >> **Activate Bank Feed**. If you don’t see the Starling logo, select the account you’re trying to link up, as above, go to the ban statement view, **More Options** >> **Settings** , and set it to Starling. Both the logo and the option to activate the feed should then appear for you.   ...  quickfile \n",
      "\n",
      "[2 rows x 12 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>post_num</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>num_reads</th>\n",
       "      <th>topic_slug</th>\n",
       "      <th>forum_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)</td>\n",
       "      <td>25656</td>\n",
       "      <td>4859</td>\n",
       "      <td>Justin_Keikhlasan</td>\n",
       "      <td>8815.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-06-03T17:57:25.143Z</td>\n",
       "      <td>2020-06-08T20:27:53.011Z</td>\n",
       "      <td>18.0</td>\n",
       "      <td>flavor-booster-ratings</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng</td>\n",
       "      <td>25662</td>\n",
       "      <td>4856</td>\n",
       "      <td>matt009</td>\n",
       "      <td>8815.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-04T03:20:52.395Z</td>\n",
       "      <td>2020-06-04T03:20:52.395Z</td>\n",
       "      <td>15.0</td>\n",
       "      <td>flavor-booster-ratings</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.</td>\n",
       "      <td>25677</td>\n",
       "      <td>4635</td>\n",
       "      <td>Tom</td>\n",
       "      <td>8815.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-06-04T17:03:37.547Z</td>\n",
       "      <td>2020-06-04T17:04:06.818Z</td>\n",
       "      <td>14.0</td>\n",
       "      <td>flavor-booster-ratings</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i tried choc cherry only one i didnt like</td>\n",
       "      <td>25678</td>\n",
       "      <td>4856</td>\n",
       "      <td>matt009</td>\n",
       "      <td>8815.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-06-04T18:19:22.386Z</td>\n",
       "      <td>2020-06-04T18:19:22.386Z</td>\n",
       "      <td>14.0</td>\n",
       "      <td>flavor-booster-ratings</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>**Mint Chocolate** \\- 5/5. Like [@Tom](/u/tom) said, we think it tastes just like you would expect. It’s my wife’s favorite way to have Huel, and also her favorite Soylent RTD flavor, which is more like a treat than a meal.  **Peanut Butter** \\- 3/5. It’s decent, but I’m pretty sure it has the same calories as peanut butter powders like PBFit and PB2, which taste more like peanut butter.  **Gingerbread** \\- 3/5. Not bad, but I don’t know that “Gingerbread” is an accurate name for the flavor. I made my own by combining the spices from a gingerbread recipe into a jar. (allspice, cinnamon, ginger, cloves, nutmeg).  **Salted Caramel** \\- 2/5. Only tried it once from the sample pack. I understand that “salted” is in the name, but I felt as if the salt flavor was too prominent. Like when too much salt from the rim falls into your magarita, and end up with margarita-flavored sea water.  **Banana** \\- If you like banana flavored stuff (candy / taffy) then you’ll most likely enjoy this. I like it just fine, and my wife wouldn’t even try it after smelling it. [@Justin_Keikhlasan](/u/justin_keikhlasan) I’ve always wondered about the Vanilla flavor boost. How did you use it? What flavor of Huel did you add it to? How did the flavor compare to Vanilla Huel?</td>\n",
       "      <td>25679</td>\n",
       "      <td>4073</td>\n",
       "      <td>ericb</td>\n",
       "      <td>8815.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-04T19:57:36.200Z</td>\n",
       "      <td>2020-06-04T20:01:21.397Z</td>\n",
       "      <td>13.0</td>\n",
       "      <td>flavor-booster-ratings</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            post_text  ... forum_name\n",
       "0  Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)                                                                                                                                                                                                                                                                                                                                                                                                                                        ...  huel     \n",
       "1  i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  huel     \n",
       "2   Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.   ...  huel     \n",
       "3  i tried choc cherry only one i didnt like                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...  huel     \n",
       "4    **Mint Chocolate** \\- 5/5. Like [@Tom](/u/tom) said, we think it tastes just like you would expect. It’s my wife’s favorite way to have Huel, and also her favorite Soylent RTD flavor, which is more like a treat than a meal.  **Peanut Butter** \\- 3/5. It’s decent, but I’m pretty sure it has the same calories as peanut butter powders like PBFit and PB2, which taste more like peanut butter.  **Gingerbread** \\- 3/5. Not bad, but I don’t know that “Gingerbread” is an accurate name for the flavor. I made my own by combining the spices from a gingerbread recipe into a jar. (allspice, cinnamon, ginger, cloves, nutmeg).  **Salted Caramel** \\- 2/5. Only tried it once from the sample pack. I understand that “salted” is in the name, but I felt as if the salt flavor was too prominent. Like when too much salt from the rim falls into your magarita, and end up with margarita-flavored sea water.  **Banana** \\- If you like banana flavored stuff (candy / taffy) then you’ll most likely enjoy this. I like it just fine, and my wife wouldn’t even try it after smelling it. [@Justin_Keikhlasan](/u/justin_keikhlasan) I’ve always wondered about the Vanilla flavor boost. How did you use it? What flavor of Huel did you add it to? How did the flavor compare to Vanilla Huel?                                  ...  huel     \n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for sheet_name in SPREADSHEET_NAMES:\n",
    "  print(f'Joining {sheet_name}')\n",
    "  df = loadgsheet(sheet_name)\n",
    "  print(df.head(2))\n",
    "  df_all = pd.concat([df_all, df]).dropna(axis=0)\n",
    "\n",
    "df_all.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1596991197914,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "qAOsASsq1xzs",
    "outputId": "c9d71091-e546-46bd-bd64-da1edf67e85f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>forum_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>Yes. I have tried beano and gas-x but they make little difference. It does seem to be waning lately but still pretty bad.I have been on huel 3.5 scoops a meal twice daily for a few months now.</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>Hi, i enrolled into atp few weeks ago… However will start classes in a few months (paid my deposit). So they are still accepting students. As for the people who complain… They are usually unmotivated and lazy type who can’t hold a job for more than a few weeks. These are the people who always show up unprepared and don’t study… And don’t care about burning through daddy’s and mommy’s 80k. If you don’t hold an essential college skill (STUDYING &amp;amp; MOTIVATION) ATP may not be for you.</td>\n",
       "      <td>airline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>They didn’t remove the badges, I thought this would count as a community competition.</td>\n",
       "      <td>hopscotch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>This topic was automatically closed after 7 days. New replies are no longer allowed.</td>\n",
       "      <td>quickfile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5784</th>\n",
       "      <td>BNZ does offer some great credit cards where you can earn rewards, just for using it! Feel free to call 0800 275 269 or pop into any BNZ store if you wanted to discuss switching. We’re always here to help ![:slight_smile:](https://community.bnz.co.nz/images/emoji/twitter/slight_smile.png?v=5)</td>\n",
       "      <td>bnz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     post_text forum_name\n",
       "1819  Yes. I have tried beano and gas-x but they make little difference. It does seem to be waning lately but still pretty bad.I have been on huel 3.5 scoops a meal twice daily for a few months now.                                                                                                                                                                                                                                                                                                          huel     \n",
       "1744  Hi, i enrolled into atp few weeks ago… However will start classes in a few months (paid my deposit). So they are still accepting students. As for the people who complain… They are usually unmotivated and lazy type who can’t hold a job for more than a few weeks. These are the people who always show up unprepared and don’t study… And don’t care about burning through daddy’s and mommy’s 80k. If you don’t hold an essential college skill (STUDYING &amp; MOTIVATION) ATP may not be for you.  airline  \n",
       "1556  They didn’t remove the badges, I thought this would count as a community competition.                                                                                                                                                                                                                                                                                                                                                                                                                     hopscotch\n",
       "261   This topic was automatically closed after 7 days. New replies are no longer allowed.                                                                                                                                                                                                                                                                                                                                                                                                                      quickfile\n",
       "5784  BNZ does offer some great credit cards where you can earn rewards, just for using it! Feel free to call 0800 275 269 or pop into any BNZ store if you wanted to discuss switching. We’re always here to help ![:slight_smile:](https://community.bnz.co.nz/images/emoji/twitter/slight_smile.png?v=5)                                                                                                                                                                                                     bnz      "
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle df\n",
    "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Sample df\n",
    "df_all[['post_text', 'forum_name']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBVWAEfQ4L5L"
   },
   "source": [
    "## 1.2 Data Cleansing and Prep\n",
    "\n",
    "Now we've loaded the data, we must remove noise from the dataset. Please explore some techniques in which we could clean the data in order to us to see how well pre-trained BERT works on our dataset. Luckily, due to the way BERT tokenises the data, we don't need to the same extent of data preprocessing as required of previous NLP models. However we still need to - \n",
    "\n",
    "1. Filter nulls\n",
    "2. Filter for duplicates\n",
    "3. [Optional] Remove post_text which does not have vocab in pre-trained BERT. Later, we will leave this in for finetuning.\n",
    "  * Hyperlinks \n",
    "  * Foreign languages - there are multilingual BERT models\n",
    "  * Any more you can think of?\n",
    "4. Encode the labels - map categorical labels to numerical values\n",
    "\n",
    "* See [here](https://drive.google.com/open?id=1PotbhjemiMobHu0Loy-mDHIumdJh-LxC) for Pandas cleaning tutorials\n",
    "* See [here](https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/#3) for beginner EDA tutorial for NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OOp_ioZKMrO"
   },
   "source": [
    "### 1.2.1 Filter nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1543,
     "status": "ok",
     "timestamp": 1596991213990,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "U84E0klnKQxq",
    "outputId": "e61b500e-ae66-42ef-9c6f-089a60766335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post_text      0\n",
       "post_id        0\n",
       "user_id        0\n",
       "username       0\n",
       "topic_id       0\n",
       "post_num       0\n",
       "reply_count    0\n",
       "created_at     0\n",
       "updated_at     0\n",
       "num_reads      0\n",
       "topic_slug     0\n",
       "forum_name     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()  # No nulls as we dropna on the fly when loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5a7ZEC1iKRD6"
   },
   "source": [
    "### 1.2.2 Filter for duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAaOi1O66_8Q"
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydk4j7iySWlH"
   },
   "source": [
    "### 1.2.3 Encode the labels\n",
    "\n",
    "We then need to encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1267,
     "status": "ok",
     "timestamp": 1596991226180,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "5dR4-AN14AOz",
    "outputId": "4c0c3900-f75b-4ac1-db73-c5fc6bd96016"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>post_num</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>num_reads</th>\n",
       "      <th>topic_slug</th>\n",
       "      <th>forum_name</th>\n",
       "      <th>forum_name_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Yeah [@naturallycured](/u/naturallycured)  Got me into D.E.N  It’s the best supplement I’ve taken  Works better than Amyloban  Although Amyloban is pretty good!  But check this out!  D.E.N strengthens the sarcosine we take as well and makes it more efficient!  It’s insane the gains I am getting from those two  Also distilled water has been working miracles for us as well with a healthy diet filled with probiotics</td>\n",
       "      <td>2.03371e+06</td>\n",
       "      <td>13939</td>\n",
       "      <td>Themoonshinemaras</td>\n",
       "      <td>182336.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-12-13T16:47:34.455Z</td>\n",
       "      <td>2019-12-13T16:47:34.455Z</td>\n",
       "      <td>92.0</td>\n",
       "      <td>dmg-the-best-supplement-for-negative-symptoms-depression-anxiety-and-improving-cognition-mood-libido-energy-workout</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>I need to be able to advise clients of the reference to quote when making online payments, which is a basically their account reference and name, but unless I’m missing something I don’t see these as custom tokens which can be added to the email message, or any other field which could be used to store a reference. (I’m not sending invoices or links to invoices from QuickFile for various reasons, not least that the recipients aren’t very computer literate, so the email itself is their bill.) Any suggestions? Thanks</td>\n",
       "      <td>86543</td>\n",
       "      <td>9534</td>\n",
       "      <td>JohnJ</td>\n",
       "      <td>24401.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-12-23T18:00:31.300Z</td>\n",
       "      <td>2019-12-19T14:04:20.662Z</td>\n",
       "      <td>21.0</td>\n",
       "      <td>client-name-and-account-reference-in-email</td>\n",
       "      <td>quickfile</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>lr10123\"&gt;LR10123 you can judge. lapislasli09\"&gt;lapislasli09 and lunalovegood\"&gt;LunaLovegood you can judge and participate but if you do both you can’t vote for your own project.</td>\n",
       "      <td>2621862</td>\n",
       "      <td>10205</td>\n",
       "      <td>ellatm</td>\n",
       "      <td>54856.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-04-25T13:24:50.689Z</td>\n",
       "      <td>2020-04-25T13:24:50.689Z</td>\n",
       "      <td>28.0</td>\n",
       "      <td>adventure-game-competition</td>\n",
       "      <td>hopscotch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>lol I hope you get that that was fake and just an example</td>\n",
       "      <td>2397617</td>\n",
       "      <td>6167</td>\n",
       "      <td>Hopscotcher</td>\n",
       "      <td>52372.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-06-14T15:28:28.306Z</td>\n",
       "      <td>2019-06-14T15:28:28.306Z</td>\n",
       "      <td>79.0</td>\n",
       "      <td>a-message-about-impersonation</td>\n",
       "      <td>hopscotch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>I have the vanilla. I didn’t like it at all the first time. I added instant coffee the next morning and that was good. Now I absolutely crave it, with or without the coffee added. Keep trying.</td>\n",
       "      <td>11417</td>\n",
       "      <td>1988</td>\n",
       "      <td>Luanne55</td>\n",
       "      <td>2888.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-10-30T22:21:11.487Z</td>\n",
       "      <td>2018-10-30T22:21:11.487Z</td>\n",
       "      <td>109.0</td>\n",
       "      <td>the-taste-is-awful-just-wretched</td>\n",
       "      <td>huel</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     post_text  ... forum_name_encoded\n",
       "850   Yeah [@naturallycured](/u/naturallycured)  Got me into D.E.N  It’s the best supplement I’ve taken  Works better than Amyloban  Although Amyloban is pretty good!  But check this out!  D.E.N strengthens the sarcosine we take as well and makes it more efficient!  It’s insane the gains I am getting from those two  Also distilled water has been working miracles for us as well with a healthy diet filled with probiotics                                                                                                          ...  5                \n",
       "1987  I need to be able to advise clients of the reference to quote when making online payments, which is a basically their account reference and name, but unless I’m missing something I don’t see these as custom tokens which can be added to the email message, or any other field which could be used to store a reference. (I’m not sending invoices or links to invoices from QuickFile for various reasons, not least that the recipients aren’t very computer literate, so the email itself is their bill.) Any suggestions? Thanks   ...  4                \n",
       "2748  lr10123\">LR10123 you can judge. lapislasli09\">lapislasli09 and lunalovegood\">LunaLovegood you can judge and participate but if you do both you can’t vote for your own project.                                                                                                                                                                                                                                                                                                                                                           ...  2                \n",
       "3568  lol I hope you get that that was fake and just an example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  2                \n",
       "5785   I have the vanilla. I didn’t like it at all the first time. I added instant coffee the next morning and that was good. Now I absolutely crave it, with or without the coffee added. Keep trying.                                                                                                                                                                                                                                                                                                                                         ...  3                \n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['forum_name'] = df_all['forum_name'].astype('category')\n",
    "df_all['forum_name_encoded'] = df_all['forum_name'].cat.codes.astype('int32')\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0VBdOxhKWCn"
   },
   "source": [
    "### 1.2.3 [Optional] Filter noise\n",
    "\n",
    "Remove post_text which does not have vocab in pre-trained BERT. Later, we will leave this in for finetuning.\n",
    "\n",
    "* Emojis\n",
    "* Hyperlinks\n",
    "* Foreign languages\n",
    "* Any more you can think of?\n",
    "\n",
    "**How do we filter for these anomalies?** \n",
    "\n",
    "Perhaps we try to split strings by space and remove that match markdown hyperlink syntax `![]()`?\n",
    "\n",
    "We can always see how BERT performs with dirty data and then perform further pre-processing as we move forward such as expanding contractions etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7619,
     "status": "ok",
     "timestamp": 1596991270410,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "gn2lcvl4X62P",
    "outputId": "da5ffbc8-aa54-485b-d9ab-17202e44a7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
      "\r",
      "\u001b[K     |██████▍                         | 10kB 25.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 20kB 26.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 30kB 18.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 40kB 13.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49714 sha256=6eef6c30515322d3006b2be087a6189be8f08e6f01bdbacac26dae478c6a8928\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n",
      "OK thanks for letting me know Matthew. I myself never struggled for space but did find the logo to be a good visual cue as to which account I was working in. Its not a show stopper though I will just have to scroll to the top and read the account name instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>post_num</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>num_reads</th>\n",
       "      <th>topic_slug</th>\n",
       "      <th>forum_name</th>\n",
       "      <th>forum_name_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>I have send a secure message on internet banking. Ta</td>\n",
       "      <td>34445</td>\n",
       "      <td>1558</td>\n",
       "      <td>Burfot</td>\n",
       "      <td>29569.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-30T00:21:48.366Z</td>\n",
       "      <td>2018-04-30T00:21:48.366Z</td>\n",
       "      <td>22.0</td>\n",
       "      <td>automatic-payments-on-joint-account</td>\n",
       "      <td>bnz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Just set up quickfile as was given a quickfile email address how to I access it to retrive password as have been logged out as went to lunch? thanks</td>\n",
       "      <td>128852</td>\n",
       "      <td>14026</td>\n",
       "      <td>lilaclady</td>\n",
       "      <td>35453.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-05-22T14:07:52.400Z</td>\n",
       "      <td>2020-05-22T14:07:52.400Z</td>\n",
       "      <td>13.0</td>\n",
       "      <td>how-do-i-access-my-quickfile-email-address</td>\n",
       "      <td>quickfile</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>But… but the 787’s so beautiful Seriously though, hope that it all works out for you, and there’s always an upside to everything. Calhoun (CEO of Boeing) said that he expects one of the major airlines to close their doors on the other side, but they can’t do anything right now, so it’s kinda pointless to beat this horse right now since the CARES Act essentially turned on the Pause button in the US, and who know’s what things’ll be like in October. Plus, doesn’t the 737 do the transcon and Caribbean trips at UA? And I’m guessing that Dulles is much easier than Newark to commute to.</td>\n",
       "      <td>74115</td>\n",
       "      <td>15895</td>\n",
       "      <td>dan</td>\n",
       "      <td>26451.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-05-12T16:18:01.848Z</td>\n",
       "      <td>2020-05-12T16:18:01.848Z</td>\n",
       "      <td>58.0</td>\n",
       "      <td>airlines-after-covid19</td>\n",
       "      <td>airline</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>Well…I´m 40. Since I was a child I´ve always wanted to become a pilot - Been addicted to everything related to airplanes and aviation - When I was on „the best age“ to start back in the late 1990s I was told I had to be extremely good in math and physics, have 100% good eye-vision and great grades - now that ship sailed for me. I went to another industry, IT and computer graphics - 8 years later, at the age of 28 I was told that 28 was little bit too old - again I listen too much to people that actually didn´t knew what they were talking about - or I didn´t think about the option to check if they were wrong. In 2016 I decided to take the class 1 medical - My health was good so I signed up for the next PPL class that started in summer 2017 - Now… 7 weeks ago on the 1st of June I completed the check ride. When the examiner shaked my hand when I shut down the engine after the flight and said \"congratulation… you are now a pilot“, I felt my eyes were about to burst into tears - I tried not to smile too much and keep it together…what a feeling - I ask my self every 2-3 days “Why didn´t I start sooner”… but, I´m so glad I did. The only bills I payed with a smile were the one from the flight school - Today it´s amazing to wake up, check the TAF and have the option to fly if the weather is good - such a privilege - But in 2 months I´ll start the ATPL which will be a much bigger fish and a huge rollercoaster… but I´ll just take one day at a time For everyone who are around the age of 40, I woud say just go for it if it´s your passion - specially if you wake up thinking about aviation and go to sleep with aviation on your mind and the pilot dream. I really don´t care what the salaries are gonna be - I just wan´t to do what I think I was ment to do and get paid for it…</td>\n",
       "      <td>38692</td>\n",
       "      <td>5610</td>\n",
       "      <td>nimbostratus</td>\n",
       "      <td>102.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-07-10T03:00:03.271Z</td>\n",
       "      <td>2018-07-10T03:03:38.158Z</td>\n",
       "      <td>286.0</td>\n",
       "      <td>is-age-40-to-old-to-start</td>\n",
       "      <td>airline</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>Hi [WicstunElectrical](https://community.quickfile.co.uk/u/WicstunElectrical), Short, yes you can. Have a look here: ![](/user_avatar/community.quickfile.co.uk/glenn/40/11888_2.png) [Custom Trading Styles](https://community.quickfile.co.uk/t/custom-trading- styles/8831) [Design](/c/knowledgebase/design) &gt; Please Note: The features described in this document requires a Power User &gt; Subscription Introduction A “Trading Style” is a particular business name &gt; and branding theme that should be applied to a company’s invoices, estimates &gt; and general stationery. Sometimes a company may have a trading style that &gt; deviates from their official registered name, for example Acme “Group &gt; Limited” trading as “Acme Printing Supplies”. When to use custom trading &gt; styles? Occasionally you may have multiple trading s…</td>\n",
       "      <td>129734</td>\n",
       "      <td>10457</td>\n",
       "      <td>rhc</td>\n",
       "      <td>35689.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-06-08T15:20:16.594Z</td>\n",
       "      <td>2020-06-08T15:20:16.594Z</td>\n",
       "      <td>9.0</td>\n",
       "      <td>trading-under-two-brand-names</td>\n",
       "      <td>quickfile</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         post_text  ... forum_name_encoded\n",
       "1703  I have send a secure message on internet banking. Ta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  1                \n",
       "960   Just set up quickfile as was given a quickfile email address how to I access it to retrive password as have been logged out as went to lunch? thanks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  4                \n",
       "2483  But… but the 787’s so beautiful Seriously though, hope that it all works out for you, and there’s always an upside to everything. Calhoun (CEO of Boeing) said that he expects one of the major airlines to close their doors on the other side, but they can’t do anything right now, so it’s kinda pointless to beat this horse right now since the CARES Act essentially turned on the Pause button in the US, and who know’s what things’ll be like in October. Plus, doesn’t the 737 do the transcon and Caribbean trips at UA? And I’m guessing that Dulles is much easier than Newark to commute to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  0                \n",
       "3818  Well…I´m 40. Since I was a child I´ve always wanted to become a pilot - Been addicted to everything related to airplanes and aviation - When I was on „the best age“ to start back in the late 1990s I was told I had to be extremely good in math and physics, have 100% good eye-vision and great grades - now that ship sailed for me. I went to another industry, IT and computer graphics - 8 years later, at the age of 28 I was told that 28 was little bit too old - again I listen too much to people that actually didn´t knew what they were talking about - or I didn´t think about the option to check if they were wrong. In 2016 I decided to take the class 1 medical - My health was good so I signed up for the next PPL class that started in summer 2017 - Now… 7 weeks ago on the 1st of June I completed the check ride. When the examiner shaked my hand when I shut down the engine after the flight and said \"congratulation… you are now a pilot“, I felt my eyes were about to burst into tears - I tried not to smile too much and keep it together…what a feeling - I ask my self every 2-3 days “Why didn´t I start sooner”… but, I´m so glad I did. The only bills I payed with a smile were the one from the flight school - Today it´s amazing to wake up, check the TAF and have the option to fly if the weather is good - such a privilege - But in 2 months I´ll start the ATPL which will be a much bigger fish and a huge rollercoaster… but I´ll just take one day at a time For everyone who are around the age of 40, I woud say just go for it if it´s your passion - specially if you wake up thinking about aviation and go to sleep with aviation on your mind and the pilot dream. I really don´t care what the salaries are gonna be - I just wan´t to do what I think I was ment to do and get paid for it…  ...  0                \n",
       "1555  Hi [WicstunElectrical](https://community.quickfile.co.uk/u/WicstunElectrical), Short, yes you can. Have a look here: ![](/user_avatar/community.quickfile.co.uk/glenn/40/11888_2.png) [Custom Trading Styles](https://community.quickfile.co.uk/t/custom-trading- styles/8831) [Design](/c/knowledgebase/design) > Please Note: The features described in this document requires a Power User > Subscription Introduction A “Trading Style” is a particular business name > and branding theme that should be applied to a company’s invoices, estimates > and general stationery. Sometimes a company may have a trading style that > deviates from their official registered name, for example Acme “Group > Limited” trading as “Acme Printing Supplies”. When to use custom trading > styles? Occasionally you may have multiple trading s…                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  4                \n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter emojis\n",
    "!pip install emoji\n",
    "\n",
    "import emoji\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "text = give_emoji_free_text(df_all['post_text'][0])\n",
    "print(text)\n",
    "\n",
    "df_all['post_text'] = df_all['post_text'].apply((lambda x: give_emoji_free_text(x)))\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6736,
     "status": "ok",
     "timestamp": 1596991280625,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "5GCYPiT2WObD",
    "outputId": "0a26092a-acc9-47e4-8f0e-66a74a2fafe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html2text\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2020.1.16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>post_num</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>num_reads</th>\n",
       "      <th>topic_slug</th>\n",
       "      <th>forum_name</th>\n",
       "      <th>forum_name_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>This topic was automatically closed after 7 days. New replies are no longer allowed.</td>\n",
       "      <td>126729</td>\n",
       "      <td>-1</td>\n",
       "      <td>system</td>\n",
       "      <td>34792.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-17T10:56:13.098Z</td>\n",
       "      <td>2020-04-17T10:56:13.098Z</td>\n",
       "      <td>2.0</td>\n",
       "      <td>tide-open-bank-feed-will-not-activate-re-activate</td>\n",
       "      <td>quickfile</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>I wish that I could say I’ve met him before, but his name doesn’t sound familiar. I’ll keep an eye out. Did he fly the Q before upgrading? Tory</td>\n",
       "      <td>73742</td>\n",
       "      <td>3788</td>\n",
       "      <td>Tory</td>\n",
       "      <td>26459.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-05-05T00:12:43.201Z</td>\n",
       "      <td>2020-05-05T00:12:43.201Z</td>\n",
       "      <td>47.0</td>\n",
       "      <td>does-anyone-know-of-student-flight-scholarships</td>\n",
       "      <td>airline</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>I suffered childhood abuse when I was younger. I was diagnosed with PTSD when I was around 14 years old.</td>\n",
       "      <td>2.24134e+06</td>\n",
       "      <td>10255</td>\n",
       "      <td>laetitia</td>\n",
       "      <td>199901.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-06-08T03:49:07.900Z</td>\n",
       "      <td>2020-06-08T03:49:07.900Z</td>\n",
       "      <td>15.0</td>\n",
       "      <td>how-do-you-know-if-you-have-ptsd-aswell-as-sz</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>Not really much available in this area. Most 135 gigs are being dried up by high qual guys that have been furloughed. Finding a Part 91 job is like needles in a haystack. It cost us a lot to move down here so we don’t exactly have the finances to uproot to another location. I’ve looked into Civil Air Patrol to supplement hours but they won’t be active until these lockdowns ease up a bit. I thought about Ameriflight to get turbine ME time, but with the airline industry tightening up, they are taking advantage of pilots by having them sign year contracts with training agreements. My own flight school had me sign a 1-year non-compete contract within 50 miles of base, so that limits my options as well… Bit of a holding pattern it would seem until things improve. Michael T.</td>\n",
       "      <td>73693</td>\n",
       "      <td>11795</td>\n",
       "      <td>mdtaft</td>\n",
       "      <td>26448.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-05-04T03:00:01.995Z</td>\n",
       "      <td>2020-05-04T03:00:01.995Z</td>\n",
       "      <td>46.0</td>\n",
       "      <td>april-20-schedule</td>\n",
       "      <td>airline</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>![](https://sjc5.discourse- cdn.com/schizophrenia/user_avatar/forum.schizophrenia.com/skinnyme/40/83195_2.png) SkinnyMe: &gt; The following are all the met goals of my life I see a lot of success there, to be honest.</td>\n",
       "      <td>2.24092e+06</td>\n",
       "      <td>950</td>\n",
       "      <td>MrSquirrel</td>\n",
       "      <td>199858.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-07T21:09:36.616Z</td>\n",
       "      <td>2020-06-07T21:09:36.616Z</td>\n",
       "      <td>59.0</td>\n",
       "      <td>how-many-of-you-have-failed-at-everything-you-ever-tried</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         post_text  ... forum_name_encoded\n",
       "2932  This topic was automatically closed after 7 days. New replies are no longer allowed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  4                \n",
       "836   I wish that I could say I’ve met him before, but his name doesn’t sound familiar. I’ll keep an eye out. Did he fly the Q before upgrading? Tory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  0                \n",
       "2935  I suffered childhood abuse when I was younger. I was diagnosed with PTSD when I was around 14 years old.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...  5                \n",
       "1678  Not really much available in this area. Most 135 gigs are being dried up by high qual guys that have been furloughed. Finding a Part 91 job is like needles in a haystack. It cost us a lot to move down here so we don’t exactly have the finances to uproot to another location. I’ve looked into Civil Air Patrol to supplement hours but they won’t be active until these lockdowns ease up a bit. I thought about Ameriflight to get turbine ME time, but with the airline industry tightening up, they are taking advantage of pilots by having them sign year contracts with training agreements. My own flight school had me sign a 1-year non-compete contract within 50 miles of base, so that limits my options as well… Bit of a holding pattern it would seem until things improve. Michael T.   ...  0                \n",
       "2154  ![](https://sjc5.discourse- cdn.com/schizophrenia/user_avatar/forum.schizophrenia.com/skinnyme/40/83195_2.png) SkinnyMe: > The following are all the met goals of my life I see a lot of success there, to be honest.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...  5                \n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter hyperlinks\n",
    "!pip install html2text\n",
    "\n",
    "import html2text\n",
    "h = html2text.HTML2Text()\n",
    "h.unicode_snob = True\n",
    "df_all['post_text'] = df_all['post_text'].apply((lambda x: h.handle(x).replace('\\n', ' ').replace('  ', ' ')))\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbHKcp60YTrs"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('words')\n",
    "# words = set(nltk.corpus.words.words())\n",
    "\n",
    "# # Filter non English posts\n",
    "\n",
    "# def remove_foreign_sentences(s):\n",
    "#   for w in nltk.wordpunct_tokenize(s):\n",
    "#     if not w.isalpha():\n",
    "#       print('Not alpha:', w)\n",
    "#       print(s.replace(w, ''))\n",
    "#     if w.lower() not in words:\n",
    "#       print('Foreign:', w)\n",
    "#       return None\n",
    "\n",
    "# test = ['1234charlene', '### asdfsafdsaf ', '*** yaas', 'yesss', 'I don\\'t knowwww, woohoo!']\n",
    "# # for s in df_all['post_text'][:10]:\n",
    "# for s in test:\n",
    "#   remove_foreign_sentences(s)\n",
    "  \n",
    "# df_all['post_text'] = df_all['post_text'].apply((lambda x: remove_foreign_sentences(x)))\n",
    "# df_all.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCNbXrFLEnsj"
   },
   "source": [
    "# 2. Forum Classifier with BERT\n",
    "\n",
    "There are two steps to creating a text classifier - \n",
    "\n",
    "1. Train NLP model to transform sentences into meaningful sentence embeddings  \n",
    "2. Train a classifier to make predictions\n",
    "\n",
    "There are many models we could use to transform our post text into meaningful sentence embeddings. However, for our project we have chosen the BERT model due to high performance on unseen data as it's been trained on large corpuses of common texts and it's ability to handle dirty data due to its tokenisation architecture. More specifically, we will be using the [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html) from the HuggingFace transformer library.\n",
    "\n",
    "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
    "* We’ll be using [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "The data we pass between the two models is a vector of size 768 (The [CLS] vector). We can think of this of vector as an embedding for the sentence that we can use for classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxf8--844cqq"
   },
   "source": [
    "## 2.1 Sentence Embeddings with BERT\n",
    "There are many flavours of BERT out there, all trained for a variety of different use cases. However, the model we are using in our project is [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) from the HuggingFace transformers library as it's simple to use.\n",
    "\n",
    "Please follow the following steps to generate a dataframe of sentence embeddings with their corresponding forum labels.\n",
    "\n",
    "1. Tokenise the sentences\n",
    "2. Pad & truncate all sentences to a single constant length for batch processing\n",
    "3. Explicitly differentiate real tokens from padding tokens with an “attention mask” \n",
    "4. Pass tokenised sentences through BERT to generate sentence embedding features\n",
    "\n",
    "\n",
    "* See this [visual starter notebook](https://colab.research.google.com/drive/1elYlJ_JKupvMJwLtuwizoYIBmwwjgaur?usp=sharing) for further understanding of how to use the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zhbTAJZnnE-"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ac5WtAFmIz0"
   },
   "source": [
    "\n",
    "Let’s extract the sentences and labels of our training set as numpy ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyct62wamJjr"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels to np array.\n",
    "sentences = df_all.post_text.values\n",
    "labels = df_all.forum_name_encoded.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtsEAN8sl77j"
   },
   "source": [
    "Let’s apply the tokenizer to one sentence just to see the output.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "When we actually convert all of our sentences, we’ll use the `tokenizer.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627,
     "referenced_widgets": [
      "cf0f9a7836e74e0d8706153c037692a8",
      "eb7bfc9777824cc3ba0fef22baa0c090",
      "74a58f609b74466bb9173e70c06a0585",
      "0232ca2058cc4826b3fd4d6a78ce1471",
      "8f88b5f19c684f30acfcc3d560b6e1ee",
      "2b1f42dd92cc43b4bd91d997cc3e5058",
      "9c34fcc1082145dcb93f69cfa0e75d50",
      "e6d5ada6f8b14364b4b8ed04815830af"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8539,
     "status": "ok",
     "timestamp": 1596991305836,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "4qk7ec9qma9C",
    "outputId": "187045f4-fa67-4fdb-9acc-412864fe2f8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0f9a7836e74e0d8706153c037692a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>forum_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>Hi there, to qualify for the 0% interest for 12 months you will need to apply for our Low Rate MasterCard. Please note you cant transfer your balance from one BNZ credit card to another. For more info about Balance Transfer you can refer to the website here . Thanks Adina.</td>\n",
       "      <td>bnz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Hey Hayden, Huel Black Edition v1.0 and Huel Powder v3.0 are different products. The core powder or v3.0 has been around since Huel started which is why it’s on version 3. Huel Black Edition is a lower carb, higher protein powder that was released Decemeber 2019 hence why it’s on version 1. Black Edition is sweetened with stevia and coconut sugar and v3.0 is sweetened with sucralose.</td>\n",
       "      <td>huel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Hi Ian, Thanks for the help ![](https://community.quickfile.co.uk/user_avatar/community.quickfile.co.uk/ian_roberts/40/17630_2.png) ian_roberts: &gt; I think we need to rewind a bit. From what you’ve said so far: &gt; &gt; * each month you pay £25 to this supplier &gt; * at some point they refunded you £100 &gt; &gt; &gt; _yes_ &gt; &gt; What is your relationship with this supplier, is it a regular monthly &gt; purchase like a subscription fee or is it more like an energy supplier where &gt; you make fixed payments each month to build up a balance that is used to &gt; settle occasional larger bills? &gt; &gt; _Simply, they charged us 25pm, when the contract ended, they continued to &gt; take funds from our account, we contacted them and they arranged for a &gt; refund, which has arrived in 2 parts 100.50 and then 19._ &gt; &gt; What exactly do you do when each of the £25 payments goes out? Do you create &gt; a separate #QFnnnn purchase for each lot of £25 or do you log the money as &gt; prepayments (pay down multiple invoices, but leave the money unallocated) &gt; and make purchases from the bills separately? &gt; &gt; _sorry not sure what you are asking, this is the process i do, i &gt; download.qif and import i then tag, when it says no corresponding invoice i &gt; let it create one_ &gt; &gt; What is the nature of the £100 refund? Are they actually crediting you to &gt; reimburse something they have invoiced you for in the past, or are they &gt; simply returning an overpaid credit balance when it turns out your monthly &gt; payments were more than they eventually billed you for? hope this helps</td>\n",
       "      <td>quickfile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>Ok, so in the project, there is a text object with a when tiltright &gt; 0 Go there, Last Touch Y should be multiplied by a number. Change the number it is multiplied by until it gives a whole number.</td>\n",
       "      <td>hopscotch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>Hi [@Matt](/u/matt) It’s likely to be down to a journal that’s affecting these figures. It can take a bit of time, but try and find out, by adjusting the dates of the report, when it last balanced. That will identify when the change took place to throw it off.</td>\n",
       "      <td>quickfile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            post_text forum_name\n",
       "3719  Hi there, to qualify for the 0% interest for 12 months you will need to apply for our Low Rate MasterCard. Please note you cant transfer your balance from one BNZ credit card to another. For more info about Balance Transfer you can refer to the website here . Thanks Adina.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                bnz      \n",
       "260   Hey Hayden, Huel Black Edition v1.0 and Huel Powder v3.0 are different products. The core powder or v3.0 has been around since Huel started which is why it’s on version 3. Huel Black Edition is a lower carb, higher protein powder that was released Decemeber 2019 hence why it’s on version 1. Black Edition is sweetened with stevia and coconut sugar and v3.0 is sweetened with sucralose.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               huel     \n",
       "289   Hi Ian, Thanks for the help ![](https://community.quickfile.co.uk/user_avatar/community.quickfile.co.uk/ian_roberts/40/17630_2.png) ian_roberts: > I think we need to rewind a bit. From what you’ve said so far: > > * each month you pay £25 to this supplier > * at some point they refunded you £100 > > > _yes_ > > What is your relationship with this supplier, is it a regular monthly > purchase like a subscription fee or is it more like an energy supplier where > you make fixed payments each month to build up a balance that is used to > settle occasional larger bills? > > _Simply, they charged us 25pm, when the contract ended, they continued to > take funds from our account, we contacted them and they arranged for a > refund, which has arrived in 2 parts 100.50 and then 19._ > > What exactly do you do when each of the £25 payments goes out? Do you create > a separate #QFnnnn purchase for each lot of £25 or do you log the money as > prepayments (pay down multiple invoices, but leave the money unallocated) > and make purchases from the bills separately? > > _sorry not sure what you are asking, this is the process i do, i > download.qif and import i then tag, when it says no corresponding invoice i > let it create one_ > > What is the nature of the £100 refund? Are they actually crediting you to > reimburse something they have invoiced you for in the past, or are they > simply returning an overpaid credit balance when it turns out your monthly > payments were more than they eventually billed you for? hope this helps   quickfile\n",
       "623   Ok, so in the project, there is a text object with a when tiltright > 0 Go there, Last Touch Y should be multiplied by a number. Change the number it is multiplied by until it gives a whole number.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            hopscotch\n",
       "2813  Hi [@Matt](/u/matt) It’s likely to be down to a journal that’s affecting these figures. It can take a bit of time, but try and find out, by adjusting the dates of the report, when it last balanced. That will identify when the change took place to throw it off.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             quickfile"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load pretrained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Sample df\n",
    "df_all[['post_text', 'forum_name']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1438,
     "status": "ok",
     "timestamp": 1596991313661,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "7zdnlHkhl6eT",
    "outputId": "6a4a9534-26cd-4caf-dc6d-90edb64099b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Seuk, Maybe his medication is on the list and maybe it’s not? And maybe the AME (who’s the professional) can recommend alternatives or other solutions. It’s not as simple as checking the list. He needs to consult an AME period. On a positive EVERY pilot I know loves giving advice on EVERYTHING. You’ll do well Adam \n",
      "Tokenized:  ['se', '##uk', ',', 'maybe', 'his', 'medication', 'is', 'on', 'the', 'list', 'and', 'maybe', 'it', '’', 's', 'not', '?', 'and', 'maybe', 'the', 'am', '##e', '(', 'who', '’', 's', 'the', 'professional', ')', 'can', 'recommend', 'alternatives', 'or', 'other', 'solutions', '.', 'it', '’', 's', 'not', 'as', 'simple', 'as', 'checking', 'the', 'list', '.', 'he', 'needs', 'to', 'consult', 'an', 'am', '##e', 'period', '.', 'on', 'a', 'positive', 'every', 'pilot', 'i', 'know', 'loves', 'giving', 'advice', 'on', 'everything', '.', 'you', '’', 'll', 'do', 'well', 'adam']\n",
      "Token IDs:  [7367, 6968, 1010, 2672, 2010, 14667, 2003, 2006, 1996, 2862, 1998, 2672, 2009, 1521, 1055, 2025, 1029, 1998, 2672, 1996, 2572, 2063, 1006, 2040, 1521, 1055, 1996, 2658, 1007, 2064, 16755, 15955, 2030, 2060, 7300, 1012, 2009, 1521, 1055, 2025, 2004, 3722, 2004, 9361, 1996, 2862, 1012, 2002, 3791, 2000, 23363, 2019, 2572, 2063, 2558, 1012, 2006, 1037, 3893, 2296, 4405, 1045, 2113, 7459, 3228, 6040, 2006, 2673, 1012, 2017, 1521, 2222, 2079, 2092, 4205]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[1])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[1]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLK2zaK6rYXV"
   },
   "source": [
    "The below cell will perform one tokenisation pass of the dataset in order to measure the maximum sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1596991356314,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "LeetVuq9rXoi",
    "outputId": "f7a89978-362c-49c3-906e-51945ef83d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  167\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "# For first 10 sentences - \n",
    "for s in sentences[:10]:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(s, add_special_tokens=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U65mn_Dgse7t"
   },
   "source": [
    "\n",
    "\n",
    "For BERT, all sentences must be padded or truncated to a single, fixed length. The maximum sentence length is 512 tokens so you may have to have to split the post_text. The maximum length does impact training and evaluation speed, however. For example, with a Tesla K80 (Colab GPU):\n",
    "\n",
    "`MAX_LEN = 128 --> Training epochs take ~5:28 each`\n",
    "\n",
    "`MAX_LEN = 64 --> Training epochs take ~2:57 each`\n",
    "\n",
    "Try and encode the the dataset with the DistilBertTokenizer below.\n",
    "\n",
    "See [docs here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode) for `tokenizer.encode` . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44845,
     "status": "ok",
     "timestamp": 1592901006967,
     "user": {
      "displayName": "Charlene_Leong stemaway",
      "photoUrl": "",
      "userId": "07260625547596395059"
     },
     "user_tz": -720
    },
    "id": "lhSh0z1ZZIYN",
    "outputId": "0ac0d061-35d8-47b8-8a64-ab7abbb6a38a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Splitting sentences over 128 strings\n",
    "\n",
    "# for s in sentences[0:100]:\n",
    "#   if len(s)>512:\n",
    "#     print(nltk.tokenize.sent_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmyhO_m7m_5H"
   },
   "source": [
    "### 2.1.2 Padding the Sentences and Attention Mask \n",
    "\n",
    "- For BERT, all sentences must be padded or truncated to a single, fixed length.\n",
    "The maximum sentence length is 512 tokens.\n",
    "- Padding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a “MAX_LEN” of 8 tokens.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"500\"/>\n",
    "\n",
    "The “Attention Mask” is simply an array of 1s and 0s indicating which tokens are padding and which aren’t (seems kind of redundant, doesn’t it?!). This mask tells the “Self-Attention” mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n",
    "\n",
    "\n",
    "Therefore the encoding task requires the following - \n",
    "\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special [CLS] and [SEP] tokens.\n",
    "3. Pad or truncate all sentences to the same length.\n",
    "4. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n",
    "5. Map the tokens to their IDs.\n",
    "\n",
    "\n",
    "You should try implement the padding and attention masks yourself with matrix multiplication via numpy. It is trained on lower-cased English text. Hence we set the flag **do_lower_case** to true in BertTokenizer.\n",
    "\n",
    "\n",
    "Otherwise, the first four features are in `tokenizer.encode`, but you can also try use the `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1295,
     "status": "error",
     "timestamp": 1596991595342,
     "user": {
      "displayName": "maleeha_imran stemaway",
      "photoUrl": "",
      "userId": "07638472317365416604"
     },
     "user_tz": 420
    },
    "id": "MQemkj2BqqJ9",
    "outputId": "99787102-3d09-49c7-c9b3-07a677eadc98"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-66bf60cb4da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mreturn_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Construct attn masks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"True\"\u001b[0m         \u001b[0;31m# Return pytorch tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                    )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m         )\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m                 )  # Default to truncate the longest sequences in pairs of inputs\n\u001b[1;32m   1498\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m                 \u001b[0mtruncation_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mtruncation_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_TRUNCATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# still not found -- try _missing_ hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_next_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_missing_\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\n\u001b[1;32m     81\u001b[0m             \u001b[0;34m\"%r is not a valid %s, please select one of %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value2member_map_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'True' is not a valid TruncationStrategy, please select one of ['only_first', 'only_second', 'longest_first', 'do_not_truncate']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "T = 128\n",
    "# For every sentence...\n",
    "for s in sentences:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start and append the `[SEP]` token to the end.\n",
    "    #   (3) Pad or truncate the sentence to `max_length`\n",
    "    #   (4) Create attention masks for [PAD] tokens\n",
    "    #   (5) Map tokens to their IDs.\n",
    "\n",
    "    # You can encode_plus as function\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        s,                              # Sentence to encode.\n",
    "                        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = T,                 # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn masks.\n",
    "                        return_tensors = 'pt',\n",
    "                                 # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    token_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9aB3l9KtUxo"
   },
   "source": [
    "## 3.1 Train the Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQYTFu_qNC5d"
   },
   "source": [
    "### 3.1.1 DistilBert For Sequence Classification\n",
    "\n",
    "\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
    "\n",
    "Thankfully, the HuggingFace Pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained DistilBERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "\n",
    "\n",
    "We’ll be using [DistilBertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/distilbert.html#distilbertforsequenceclassification). This is the normal DistilBERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "We then pass the sentence embeddings and features through to the linear regression model to evaluate forum predictions.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n",
    "\n",
    "<!-- \n",
    "1. Append the classification  layer to the BERT model\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n",
    "\n",
    "### [Optional] Grid Search for Parameters\n",
    "We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularisation strength. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1593137667548,
     "user": {
      "displayName": "melissa stemaway",
      "photoUrl": "",
      "userId": "16104067195919431167"
     },
     "user_tz": 300
    },
    "id": "UBTrCeZ6l1OC",
    "outputId": "1b817f12-ec50-45e1-f032-3e91f49f2f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,355 training samples\n",
      "  595 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(token_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'{train_size:>5,} training samples')\n",
    "print(f'{val_size:>5,} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhtNln7harfO"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "413964b0f51f4e0c8d9f3d4e49eb6411",
      "e5c9be13f1c74df2b8d624f95dd169b0",
      "97e6ffef9a9a4fe986faa27f4df91742",
      "fba000657ea34da2b124cfa98a3c6515",
      "3afa8a7710ce466f8413b5f869e3a5a1",
      "e29a0c9f33a14340a1c8f29ee0c5b4cd",
      "f01b8e13ce4d475ba1c0a332657b778d",
      "43b9111dfc8d4cca953219676152eef0",
      "dfe3ace0b1cf4f7aaa770054ba9889ae",
      "9b51e1c93b9640929ae881275528a627",
      "8e3e6a6ffe2341c6a83714f04101aa8d",
      "56de60364f5c40c0b69aa09f5d6db99c",
      "63dee3c7d3204fbf8cb90b6f9b5684d7",
      "fcc79b54c5b84a9abd83333c6b9bec14",
      "a00ac0487cf041d1bbbef37734e19e39",
      "cc8b3dbd733b48eab491410c61dfcb2e"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25948,
     "status": "ok",
     "timestamp": 1593137700848,
     "user": {
      "displayName": "melissa stemaway",
      "photoUrl": "",
      "userId": "16104067195919431167"
     },
     "user_tz": 300
    },
    "id": "3c1s0qjqeAj5",
    "outputId": "7d9dbc2d-46cb-4571-f0eb-7b7805baca1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413964b0f51f4e0c8d9f3d4e49eb6411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe3ace0b1cf4f7aaa770054ba9889ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load DistilBertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU. Make sure you enable the runtime clicking [Runtime]->[Change Runtime Type]->[Hardware Accelerator]->GPU->[Save]\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1593137706769,
     "user": {
      "displayName": "melissa stemaway",
      "photoUrl": "",
      "userId": "16104067195919431167"
     },
     "user_tz": 300
    },
    "id": "7dgr40_WPatk",
    "outputId": "b28405b0-a97a-488d-a7ae-c1fde80e2a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 104 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "distilbert.embeddings.word_embeddings.weight            (30522, 768)\n",
      "distilbert.embeddings.position_embeddings.weight          (512, 768)\n",
      "distilbert.embeddings.LayerNorm.weight                        (768,)\n",
      "distilbert.embeddings.LayerNorm.bias                          (768,)\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight     (768, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias           (768,)\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight     (768, 768)\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias           (768,)\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight     (768, 768)\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias           (768,)\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight   (768, 768)\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias         (768,)\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight           (768,)\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias             (768,)\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight           (3072, 768)\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias                 (3072,)\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight           (768, 3072)\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias                  (768,)\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight       (768,)\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias         (768,)\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight     (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pre_classifier.weight                                     (768, 768)\n",
      "pre_classifier.bias                                           (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0xZ29SHQtzx"
   },
   "source": [
    "### 3.1.2 Optimizer & Learning Rate Scheduler\n",
    "\n",
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper):\n",
    "\n",
    "* Batch size: 16, 32\n",
    "* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "* Number of epochs: 2, 3, 4\n",
    "\n",
    "We chose:\n",
    "\n",
    "* Batch size: 32 (set when creating our DataLoaders)\n",
    "* Learning rate: 2e-5\n",
    "* Epochs: 4 (we’ll see that this is probably too many…)\n",
    "\n",
    "\n",
    "The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation” (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in run_glue.py [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6s03rxX0QJbs"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6ciijeVRPPr"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wcGtckVDQRih"
   },
   "source": [
    "Below is our training loop. There’s a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase.\n",
    "\n",
    "**Training:**\n",
    "\n",
    "* Unpack our data inputs and labels\n",
    "* Load data onto the GPU for acceleration\n",
    "* Clear out the gradients calculated in the previous pass.\n",
    "* In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
    "* Forward pass (feed input data through the network)\n",
    "* Backward pass (backpropagation)\n",
    "* Tell the network to update parameters with optimizer.step()\n",
    "* Track variables for monitoring progress\n",
    "\n",
    "**Evalution:**\n",
    "* Unpack our data inputs and labels\n",
    "* Load data onto the GPU for acceleration\n",
    "* Forward pass (feed input data through the network)\n",
    "* Compute loss on our validation data and track variables for monitoring progress\n",
    "\n",
    "Pytorch hides all of the detailed calculations from us, but we’ve commented the code to point out which of the above steps are happening on each line.\n",
    "\n",
    "Define a helper function for calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJoUD6DkQL0J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3-MSfTnQOyk"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "# Helper function for formatting elapsed times as hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rm2qno-K4HZz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqG7FzRVFEIv"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1593137729636,
     "user": {
      "displayName": "melissa stemaway",
      "photoUrl": "",
      "userId": "16104067195919431167"
     },
     "user_tz": 300
    },
    "id": "oYsV4H8fCpZ-",
    "outputId": "4473f397-5b8b-48ba-8fdd-bd8201b5ea9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1628,
     "status": "error",
     "timestamp": 1593137733860,
     "user": {
      "displayName": "melissa stemaway",
      "photoUrl": "",
      "userId": "16104067195919431167"
     },
     "user_tz": 300
    },
    "id": "6J-FYdx6nFE_",
    "outputId": "805c81e0-b1f5-4c34-c2c3-4b1935910d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bc88256354ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#   [1]: attention masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#   [2]: labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mb_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b_input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        print(b_input_ids.shape)\n",
    "        print(b_input_ids.shape)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "       \n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids,\n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMqy9fnq_zvN"
   },
   "source": [
    "Try using Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUI9UG444hr6"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    glue_compute_metrics,\n",
    "    glue_output_modes,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXOVm1lx_ueB"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coBeNrYK_vAV"
   },
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"distilbert-base-cased\",\n",
    ")\n",
    "data_args = DataTrainingArguments(task_name=\"mnli\", data_dir=\"./glue_data/MNLI\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/model_name\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    per_gpu_eval_batch_size=128,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=500,\n",
    "    logging_first_step=True,\n",
    "    save_steps=1000,\n",
    "    evaluate_during_training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ids01riEAPYy"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1348,
     "status": "error",
     "timestamp": 1593137773018,
     "user": {
      "displayName": "melissa stemaway",
      "photoUrl": "",
      "userId": "16104067195919431167"
     },
     "user_tz": 300
    },
    "id": "yQz0jwnMAPuW",
    "outputId": "79586184-ab65-4e5b-8bd1-b3a9f2901f0b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-aeba4894a3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jNxp0DNKKSZ"
   },
   "source": [
    "Made by Charlene Leong"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Forum_Classifier_Starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0232ca2058cc4826b3fd4d6a78ce1471": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6d5ada6f8b14364b4b8ed04815830af",
      "placeholder": "​",
      "style": "IPY_MODEL_9c34fcc1082145dcb93f69cfa0e75d50",
      "value": " 232k/232k [00:00&lt;00:00, 323kB/s]"
     }
    },
    "2b1f42dd92cc43b4bd91d997cc3e5058": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3afa8a7710ce466f8413b5f869e3a5a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "413964b0f51f4e0c8d9f3d4e49eb6411": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_97e6ffef9a9a4fe986faa27f4df91742",
       "IPY_MODEL_fba000657ea34da2b124cfa98a3c6515"
      ],
      "layout": "IPY_MODEL_e5c9be13f1c74df2b8d624f95dd169b0"
     }
    },
    "43b9111dfc8d4cca953219676152eef0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56de60364f5c40c0b69aa09f5d6db99c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc8b3dbd733b48eab491410c61dfcb2e",
      "placeholder": "​",
      "style": "IPY_MODEL_a00ac0487cf041d1bbbef37734e19e39",
      "value": " 268M/268M [00:03&lt;00:00, 74.7MB/s]"
     }
    },
    "63dee3c7d3204fbf8cb90b6f9b5684d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "74a58f609b74466bb9173e70c06a0585": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b1f42dd92cc43b4bd91d997cc3e5058",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f88b5f19c684f30acfcc3d560b6e1ee",
      "value": 231508
     }
    },
    "8e3e6a6ffe2341c6a83714f04101aa8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcc79b54c5b84a9abd83333c6b9bec14",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63dee3c7d3204fbf8cb90b6f9b5684d7",
      "value": 267967963
     }
    },
    "8f88b5f19c684f30acfcc3d560b6e1ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "97e6ffef9a9a4fe986faa27f4df91742": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e29a0c9f33a14340a1c8f29ee0c5b4cd",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3afa8a7710ce466f8413b5f869e3a5a1",
      "value": 442
     }
    },
    "9b51e1c93b9640929ae881275528a627": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c34fcc1082145dcb93f69cfa0e75d50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a00ac0487cf041d1bbbef37734e19e39": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc8b3dbd733b48eab491410c61dfcb2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf0f9a7836e74e0d8706153c037692a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74a58f609b74466bb9173e70c06a0585",
       "IPY_MODEL_0232ca2058cc4826b3fd4d6a78ce1471"
      ],
      "layout": "IPY_MODEL_eb7bfc9777824cc3ba0fef22baa0c090"
     }
    },
    "dfe3ace0b1cf4f7aaa770054ba9889ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e3e6a6ffe2341c6a83714f04101aa8d",
       "IPY_MODEL_56de60364f5c40c0b69aa09f5d6db99c"
      ],
      "layout": "IPY_MODEL_9b51e1c93b9640929ae881275528a627"
     }
    },
    "e29a0c9f33a14340a1c8f29ee0c5b4cd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c9be13f1c74df2b8d624f95dd169b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6d5ada6f8b14364b4b8ed04815830af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb7bfc9777824cc3ba0fef22baa0c090": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f01b8e13ce4d475ba1c0a332657b778d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fba000657ea34da2b124cfa98a3c6515": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43b9111dfc8d4cca953219676152eef0",
      "placeholder": "​",
      "style": "IPY_MODEL_f01b8e13ce4d475ba1c0a332657b778d",
      "value": " 442/442 [00:01&lt;00:00, 257B/s]"
     }
    },
    "fcc79b54c5b84a9abd83333c6b9bec14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
