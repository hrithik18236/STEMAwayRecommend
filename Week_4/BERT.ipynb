{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stratified_Ten_Class_Forum_Classifier_Starter.ipynb","provenance":[{"file_id":"1Oq42-AXS5rZ9EGSr5epf_O5oY_T0frfp","timestamp":1597392910432},{"file_id":"1pesdv11we3_189mhIhMafAzWxk1DLdHq","timestamp":1597383646970},{"file_id":"13LS7SsLV3h3U5z4p_h38JTe9T8IF_E0e","timestamp":1597080675831}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vg1lWugC3gWZ","colab_type":"text"},"source":["# 1. Exploratory Data Analysis\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pFagFrw-gj39","colab_type":"text"},"source":["## 1.1 Let's load our data\n","\n","I have loaded from Google sheets below as the raw CSVs needed a little extra cleaning, so I did this in Sheets vs loading the CSVs and then cleaning in Pandas (just because it was quicker)! :) \n","\n","However, please feel free to experiment loading your data in a different way! :) "]},{"cell_type":"code","metadata":{"id":"z12OxhRJfZcR","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"status":"ok","timestamp":1597684224434,"user_tz":420,"elapsed":6616,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"bfd9fe22-4767-47ab-eb16-3cdce11f43da"},"source":["!pip install transformers torch pandas gspread gspread-dataframe urlextract emoji langdetect"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n","Requirement already satisfied: gspread in /usr/local/lib/python3.6/dist-packages (3.0.1)\n","Requirement already satisfied: gspread-dataframe in /usr/local/lib/python3.6/dist-packages (3.0.7)\n","Requirement already satisfied: urlextract in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.6/dist-packages (from urlextract) (1.4.4)\n","Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from urlextract) (2.10)\n","Requirement already satisfied: uritools in /usr/local/lib/python3.6/dist-packages (from urlextract) (3.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XVYhvlQL0P_r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597684224435,"user_tz":420,"elapsed":6605,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"aad106be-2283-44f9-d4e0-5eca7442410f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gI7RzqbQz761","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684224732,"user_tz":420,"elapsed":6893,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["import pandas as pd\n","import numpy as np\n","import threading, queue\n","import urlextract\n","import nltk\n","import time\n","import os\n","import re\n","\n","from langdetect import detect"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FHEeL5TgCD7","colab_type":"code","cellView":"both","colab":{},"executionInfo":{"status":"ok","timestamp":1597684224733,"user_tz":420,"elapsed":6887,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["official_forum_names = {\n","    'cartalk'      : 'Car Talk Community',\n","    'amazon'       : 'Amazon Seller Forums',\n","    'tnation'      : 'Forums - T Nation',\n","    'episode'      : 'Episode Forums',\n","    'github'       : 'GitHub Support Community',\n","    'l1t'          : 'Level1Techs Forums',\n","    'smartthings'  : 'SmartThings Community',\n","    'gearbox'      : 'The Official Gearbox Software Forums',\n","    'elasticstack' : 'Discuss the Elastic Stack',\n","    'revolut'      : 'Revolut Community'\n","}\n","\n","ROOT = '/content/gdrive/My Drive/'\n","INP_FOLDER = 'data'\n","url_extractor = urlextract.URLExtract()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"EusFWU6IA-EJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684224734,"user_tz":420,"elapsed":6881,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["def get_files(file_name_pattern=r'(?s).*', folder=INP_FOLDER):\n","  \"\"\"\n","  Returns a list of all files that match `file_name_pattern` in `folder`.\n","  \"\"\"\n","  files = []\n","  for file_name in os.listdir(os.path.join(ROOT, folder)):\n","    if re.search(file_name_pattern, file_name):\n","      files.append(os.path.join(folder, file_name))\n","  return files\n","\n","def load_posts(forum_names=official_forum_names, folder=INP_FOLDER):\n","  \"\"\"\n","  Returns a dictionary where the keys are those specified in \n","  `forum_names` and the values are dataframes that contain the associated\n","  post data for each forum in `forum_names`.\n","  \"\"\"\n","  posts = dict()\n","  files = [get_files(f'{n}_posts.csv')[0] for n in forum_names]\n","  for forum, fname in zip(forum_names, files):\n","    posts[forum] = pd.read_csv(os.path.join(ROOT, fname), index_col=0).fillna('')\n","  return posts"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAOsASsq1xzs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684237227,"user_tz":420,"elapsed":19368,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["df_all = pd.concat(load_posts().values())"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKZT1BzWIRcx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597684237228,"user_tz":420,"elapsed":19360,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"a16ccfea-250e-4e08-86fc-da90b6ab42d8"},"source":["df_all.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(692390, 15)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"a2WO6p8o0f5J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1597684237498,"user_tz":420,"elapsed":19617,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"e954519b-ac42-44e8-bb57-0535908da7e7"},"source":["df_all[['text', 'forum']].sample(5)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>forum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>118496</th>\n","      <td>Yes I have, it’s @epi.verve (like here)</td>\n","      <td>Episode Forums</td>\n","    </tr>\n","    <tr>\n","      <th>73351</th>\n","      <td>Hey everyone, I’m 40 years old 6’1” 210lbs and...</td>\n","      <td>Forums - T Nation</td>\n","    </tr>\n","    <tr>\n","      <th>107957</th>\n","      <td>yea, they are playing on normal level 40 to sh...</td>\n","      <td>The Official Gearbox Software Forums</td>\n","    </tr>\n","    <tr>\n","      <th>84991</th>\n","      <td>Have several laser sploders in different eleme...</td>\n","      <td>The Official Gearbox Software Forums</td>\n","    </tr>\n","    <tr>\n","      <th>108692</th>\n","      <td></td>\n","      <td>The Official Gearbox Software Forums</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     text                                 forum\n","118496           Yes I have, it’s @epi.verve (like here)                         Episode Forums\n","73351   Hey everyone, I’m 40 years old 6’1” 210lbs and...                     Forums - T Nation\n","107957  yea, they are playing on normal level 40 to sh...  The Official Gearbox Software Forums\n","84991   Have several laser sploders in different eleme...  The Official Gearbox Software Forums\n","108692                                                     The Official Gearbox Software Forums"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"jBVWAEfQ4L5L","colab_type":"text"},"source":["## 1.2 Data Cleansing and Prep\n","\n","Now we've loaded the data, we must remove noise from the dataset. Please explore some techniques in which we could clean the data in order for us to see how well pre-trained BERT works on our dataset. Luckily, due to the way BERT tokenises the data, we don't need to the same extent of data preprocessing as required of previous NLP models. However we still need to - \n","\n","1. Filter nulls\n","2. Filter for duplicates\n","3. [Optional] Remove post_text which does not have vocab in pre-trained BERT. Later, we will leave this in for finetuning.\n","  * Hyperlinks \n","  * Foreign languages - there are multilingual BERT models\n","  * Any more you can think of?\n","4. Encode the labels - map categorical labels to numerical values\n","\n","* See [here](https://drive.google.com/open?id=1PotbhjemiMobHu0Loy-mDHIumdJh-LxC) for Pandas cleaning tutorials\n","* See [here](https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/#3) for beginner EDA tutorial for NLP\n"]},{"cell_type":"markdown","metadata":{"id":"_OOp_ioZKMrO","colab_type":"text"},"source":["### 1.2.1 Filter nulls\n"]},{"cell_type":"code","metadata":{"id":"U84E0klnKQxq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1597684237499,"user_tz":420,"elapsed":19605,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"1c2b26e2-5d26-451a-cc0c-b47fe20ef02a"},"source":["# There shouldn't be any nulls bc they were filtered when loading\n","df_all.isnull().sum()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["post_id              0\n","username             0\n","created_at           0\n","cooked               0\n","post_num             0\n","updated_at           0\n","reply_count          0\n","reply_to_post_num    0\n","reads                0\n","topic_id             0\n","user_id              0\n","topic_slug           0\n","type                 0\n","forum                0\n","text                 0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"5a7ZEC1iKRD6","colab_type":"text"},"source":["### 1.2.2 Filter for duplicates\n"]},{"cell_type":"code","metadata":{"id":"bAaOi1O66_8Q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684242277,"user_tz":420,"elapsed":24374,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["df_all = df_all.drop_duplicates()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydk4j7iySWlH","colab_type":"text"},"source":["### 1.2.4 Encode the labels\n","\n","We then need to encode the labels."]},{"cell_type":"code","metadata":{"id":"5dR4-AN14AOz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":581},"executionInfo":{"status":"ok","timestamp":1597684242539,"user_tz":420,"elapsed":24626,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"ac316866-574c-4a36-d0ad-85f72e058319"},"source":["df_all['forum'] = df_all['forum'].astype('category')\n","df_all['forum_name_encoded'] = df_all['forum'].cat.codes.astype('int32')\n","df_all.sample(5)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>post_id</th>\n","      <th>username</th>\n","      <th>created_at</th>\n","      <th>cooked</th>\n","      <th>post_num</th>\n","      <th>updated_at</th>\n","      <th>reply_count</th>\n","      <th>reply_to_post_num</th>\n","      <th>reads</th>\n","      <th>topic_id</th>\n","      <th>user_id</th>\n","      <th>topic_slug</th>\n","      <th>type</th>\n","      <th>forum</th>\n","      <th>text</th>\n","      <th>forum_name_encoded</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>102496</th>\n","      <td>7378890</td>\n","      <td>LynnAnn</td>\n","      <td>2020-07-28T12:29:48.525Z</td>\n","      <td>&lt;p&gt;Hey! Thanks for this thread. Here are my st...</td>\n","      <td>15</td>\n","      <td>2020-07-28T12:29:48.525Z</td>\n","      <td>0</td>\n","      <td></td>\n","      <td>18</td>\n","      <td>421580</td>\n","      <td>194415</td>\n","      <td>story-recommendation-drop-yours</td>\n","      <td>latest</td>\n","      <td>Episode Forums</td>\n","      <td>Hey! Thanks for this thread. Here are my stori...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>31418</th>\n","      <td>13636</td>\n","      <td>darxtrix</td>\n","      <td>2018-11-28T15:19:23.020Z</td>\n","      <td>&lt;p&gt;Hi,&lt;/p&gt;\\n&lt;p&gt;I am not able to find the query...</td>\n","      <td>1</td>\n","      <td>2020-05-23T05:33:32.359Z</td>\n","      <td>0</td>\n","      <td></td>\n","      <td>1</td>\n","      <td>13633</td>\n","      <td>20603</td>\n","      <td>creating-a-github-release-using-github-api-v4</td>\n","      <td>latest</td>\n","      <td>GitHub Support Community</td>\n","      <td>Hi,\\nI am not able to find the query/mutations...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>39712</th>\n","      <td>4412052</td>\n","      <td>tj_s_emporium</td>\n","      <td>2018-08-03T23:48:48.342Z</td>\n","      <td>&lt;aside class=\"quote no-group\" data-post=\"4\" da...</td>\n","      <td>14</td>\n","      <td>2018-08-03T23:51:31.530Z</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>271</td>\n","      <td>390324</td>\n","      <td>70251</td>\n","      <td>separate-husband-and-wife-seller-accounts-for-...</td>\n","      <td>top</td>\n","      <td>Amazon Seller Forums</td>\n","      <td>\\n\\n\\n goodseller_2483:\\n\\njust change the SS ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>98138</th>\n","      <td>5946656</td>\n","      <td>nat_zero_six</td>\n","      <td>2020-06-25T23:19:34.410Z</td>\n","      <td>&lt;p&gt;“we are back” - melee amara enthusiast.&lt;/p&gt;</td>\n","      <td>4</td>\n","      <td>2020-06-25T23:27:56.598Z</td>\n","      <td>0</td>\n","      <td></td>\n","      <td>148</td>\n","      <td>4539628</td>\n","      <td>1501535</td>\n","      <td>how-phase-2-action-skill-melee-damage-scaling-...</td>\n","      <td>latest</td>\n","      <td>The Official Gearbox Software Forums</td>\n","      <td>“we are back” - melee amara enthusiast.</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>76354</th>\n","      <td>6602822</td>\n","      <td>GO-Logan</td>\n","      <td>2020-05-03T23:17:58.614Z</td>\n","      <td>&lt;p&gt;I can’t help you with that so I would try a...</td>\n","      <td>8</td>\n","      <td>2020-05-03T23:17:58.614Z</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>39</td>\n","      <td>373543</td>\n","      <td>186258</td>\n","      <td>bathroom-mirror-reflection-scene-help</td>\n","      <td>top</td>\n","      <td>Episode Forums</td>\n","      <td>I can’t help you with that so I would try and ...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        post_id  ... forum_name_encoded\n","102496  7378890  ...                  3\n","31418     13636  ...                  5\n","39712   4412052  ...                  0\n","98138   5946656  ...                  9\n","76354   6602822  ...                  3\n","\n","[5 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Y0VBdOxhKWCn","colab_type":"text"},"source":["### 1.2.3 [Optional] Filter noise\n","\n","Remove post_text which does not have vocab in pre-trained BERT. Later, we will leave this in for finetuning.\n","\n","* Emojis\n","* Hyperlinks\n","* Foreign languages\n","* Any more you can think of?\n","\n","**How do we filter for these anomalies?** \n","\n","Perhaps we try to split strings by space and remove that match markdown hyperlink syntax `![]()`?\n","\n","We can always see how BERT performs with dirty data and then perform further pre-processing as we move forward such as expanding contractions etc.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kUX2xsJK5NO0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684242542,"user_tz":420,"elapsed":24613,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["def replace_urls(text, repl=''):\n","  urls = list(set(url_extractor.find_urls(text)))\n","  urls.sort(key=lambda url: len(url), reverse=True)\n","  for url in urls:\n","    text = text.replace(url, repl)\n","  return text\n","\n","def is_english(text):\n","  return len(text) >= 3 and detect(text) == 'en'\n","\n","def remove_noise_serial(df, verbose=False):\n","  copy = df.copy()\n","  if verbose: print(\"Removing hyperlinks...\", end='')\n","  copy['text'] = copy['text'].apply(replace_urls)\n","  if verbose: print(\"Done!\")\n","  if verbose: print(\"Removing non-english posts...\", end='')\n","  copy = copy[copy['text'].str.replace(r'[^A-Za-z]+', ' ').apply(is_english)]\n","  if verbose: print(\"Done!\")\n","  if verbose: print(\"Removing emojis and symbols...\", end='')\n","  copy['text'] = copy['text'].str.encode('ascii', 'ignore')\\\n","                             .str.decode('ascii')\\\n","                             .str.replace(r'[^A-Za-z0-9]+', ' ')\\\n","                             .str.split()\\\n","                             .str.join(\" \")\n","  if verbose: print(\"Done!\")\n","  return copy\n","\n","def report_progress(q, event, time_between_reports=5):\n","  qsize_init = q.qsize()\n","  last_report_time = time.time()\n","  while not q.empty() and event.is_set():\n","    time_elapsed = time.time() - last_report_time\n","    if time_elapsed >= time_between_reports:\n","      portion_finished = 1 - q.qsize() / qsize_init\n","      print(\"\\tProgress: {:.1%}\".format(portion_finished))\n","      last_report_time = time.time()\n","    time.sleep(time_between_reports)\n","\n","def task(q, lock, event, file_path):\n","  while q.qsize() > 0 and event.is_set():\n","    out = remove_noise_serial(q.get())\n","    lock.acquire()\n","    out.to_csv(file_path, mode='a', header=False, index=False)\n","    lock.release()\n","    q.task_done()\n","\n","def remove_noise_parallel(df, num_threads, chunk_size, file_path, verbose=False, report_freq=5):\n","  run_event = threading.Event()\n","  run_event.set()\n","  lock = threading.Lock()\n","  work_q = queue.Queue()\n","  threads = []\n","\n","  if verbose: print('Placing tasks in work queue...', end='')\n","  for d in np.array_split(df, chunk_size):\n","    work_q.put(d)\n","  if verbose: print('\\tdone!')\n","\n","  if verbose: print('Starting threads...', end='')\n","  for i in range(num_threads):\n","    threads.append(threading.Thread(target=task, name=f'thread {i}', args=(work_q, lock, run_event, file_path, )))\n","    threads[i].start()\n","  if verbose: print('\\t\\tdone!')\n","\n","  if verbose: print(\"Processing data:\")\n","  try:\n","    if verbose: \n","      report_progress(work_q, run_event, report_freq)\n","    work_q.join()\n","  except (KeyboardInterrupt, SystemExit):\n","    run_event.clear()\n","    for t in threads:\n","      t.join()\n","  if verbose: print(\"\\nExiting!\")\n","  \n","  if any([t.is_alive() for t in threads]):\n","    print(\"WARNING: some threads may still be active!\")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IySTjDqHc0J","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684242544,"user_tz":420,"elapsed":24605,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["# # Set up csv file (this will delete any existing csv file with the same name)\n","file_name = \"bert_data.csv\"\n","file_path = os.path.join(ROOT, 'Colab Notebooks', 'BERT', file_name)\n","# if os.path.exists(file_path):\n","#   os.remove(file_path)\n","# pd.DataFrame(columns=df_all.columns).to_csv(file_path, index=False)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ib50KZl8-8kv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684242545,"user_tz":420,"elapsed":24596,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["# # No need to run this again, the file has been saved\n","# cleaned_df = remove_noise_parallel(df_all, 1500, df_all.shape[0], file_path, verbose=True, report_freq=30)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGq0df4mL1hJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684252923,"user_tz":420,"elapsed":34966,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["cleaned_df = pd.read_csv(file_path)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHJciTktcxVw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597684252924,"user_tz":420,"elapsed":34957,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"0834df5e-de3d-4c26-98da-a3242d6373a4"},"source":["cleaned_df['forum'].value_counts()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["The Official Gearbox Software Forums    110797\n","Episode Forums                          105872\n","Amazon Seller Forums                    102445\n","Forums - T Nation                        91721\n","Car Talk Community                       76723\n","Discuss the Elastic Stack                51539\n","GitHub Support Community                 31494\n","SmartThings Community                    29821\n","Revolut Community                        29297\n","Level1Techs Forums                       28475\n","Name: forum, dtype: int64"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"BXV7rrmWC2aX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684254317,"user_tz":420,"elapsed":36321,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["samples_per_forum = 20000\n","forums = []\n","for f in official_forum_names.values():\n","  forums.append(cleaned_df[cleaned_df['forum'] == f].sample(samples_per_forum, random_state=42))\n","sampled_data = pd.concat(forums).sample(frac=1, random_state=42)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMVm5ZMoDx58","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597684254319,"user_tz":420,"elapsed":36309,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"587b227d-c139-449a-9c27-5d6329500cc4"},"source":["sampled_data.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200000, 16)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"mCNbXrFLEnsj","colab_type":"text"},"source":["# 2. Forum Classifier with BERT\n","\n","There are two steps to creating a text classifier - \n","\n","1. Train NLP model to transform sentences into meaningful sentence embeddings  \n","2. Train a classifier to make predictions\n","\n","There are many models we could use to transform our post text into meaningful sentence embeddings. However, for our project we have chosen the BERT model due to high performance on unseen data as it's been trained on large corpuses of common texts and it's ability to handle dirty data due to its tokenisation architecture. More specifically, we will be using the [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html) from the HuggingFace transformer library.\n","\n","* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n","* We’ll be using [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","The data we pass between the two models is a vector of size 768 (The [CLS] vector). We can think of this of vector as an embedding for the sentence that we can use for classification. \n"]},{"cell_type":"markdown","metadata":{"id":"jxf8--844cqq","colab_type":"text"},"source":["## 2.1 Sentence Embeddings with BERT\n","There are many flavours of BERT out there, all trained for a variety of different use cases. However, the model we are using in our project is [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) from the HuggingFace transformers library as it's simple to use.\n","\n","Please follow the following steps to generate a dataframe of sentence embeddings with their corresponding forum labels.\n","\n","1. Tokenise the sentences\n","2. Pad & truncate all sentences to a single constant length for batch processing\n","3. Explicitly differentiate real tokens from padding tokens with an “attention mask” \n","4. Pass tokenised sentences through BERT to generate sentence embedding features\n","\n","\n","* See this [visual starter notebook](https://colab.research.google.com/drive/1elYlJ_JKupvMJwLtuwizoYIBmwwjgaur?usp=sharing) for further understanding of how to use the BERT model."]},{"cell_type":"markdown","metadata":{"id":"2zhbTAJZnnE-","colab_type":"text"},"source":["<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />"]},{"cell_type":"markdown","metadata":{"id":"1Ac5WtAFmIz0","colab_type":"text"},"source":["\n","Let’s extract the sentences and labels of our training set as numpy ndarrays."]},{"cell_type":"code","metadata":{"id":"oyct62wamJjr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684254320,"user_tz":420,"elapsed":36296,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["# Get the lists of sentences and their labels to np array.\n","sentences = sampled_data.text.values\n","labels = sampled_data.forum_name_encoded.values"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VtsEAN8sl77j","colab_type":"text"},"source":["Let’s apply the tokenizer to one sentence just to see the output.\n","\n","---\n","\n","\n","\n","When we actually convert all of our sentences, we’ll use the `tokenizer.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately."]},{"cell_type":"code","metadata":{"id":"4qk7ec9qma9C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597684256937,"user_tz":420,"elapsed":38901,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"14dd26a4-8912-4e06-dcc2-0d2b02aafb56"},"source":["from transformers import DistilBertTokenizer\n","\n","# Load pretrained DistilBERT tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Sample df\n","sampled_data[['text', 'forum']].sample(5)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>forum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>411279</th>\n","      <td>did you modified your grub config Use video ef...</td>\n","      <td>Level1Techs Forums</td>\n","    </tr>\n","    <tr>\n","      <th>110008</th>\n","      <td>Prior to Amazon instituting the Automated retu...</td>\n","      <td>Amazon Seller Forums</td>\n","    </tr>\n","    <tr>\n","      <th>620677</th>\n","      <td>how frequently your snapshot is scheduled to r...</td>\n","      <td>Discuss the Elastic Stack</td>\n","    </tr>\n","    <tr>\n","      <th>546798</th>\n","      <td>The closest thing to a tank in this game is FL...</td>\n","      <td>The Official Gearbox Software Forums</td>\n","    </tr>\n","    <tr>\n","      <th>166797</th>\n","      <td>Ha I am in Colorado For some reason none of th...</td>\n","      <td>Amazon Seller Forums</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     text                                 forum\n","411279  did you modified your grub config Use video ef...                    Level1Techs Forums\n","110008  Prior to Amazon instituting the Automated retu...                  Amazon Seller Forums\n","620677  how frequently your snapshot is scheduled to r...             Discuss the Elastic Stack\n","546798  The closest thing to a tank in this game is FL...  The Official Gearbox Software Forums\n","166797  Ha I am in Colorado For some reason none of th...                  Amazon Seller Forums"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"7zdnlHkhl6eT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1597684256939,"user_tz":420,"elapsed":38884,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"20513e6a-55cd-4868-f6ae-9046d95325c6"},"source":["# Print the original sentence.\n","print(' Original: ', sentences[1])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[1]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[1])))"],"execution_count":21,"outputs":[{"output_type":"stream","text":[" Original:  Insta nevaepi Name Nevada Body Neutral 04 Brow Arched Natural black Hair Two Braids Green Eyes Deepest downturn Aqua Blue Face Diamond Nose Defined Natural Lips Full Round Pouty Deep Re Matte Outfit Put her in something badass like a leather jacket Role Can she be a friend of the MC if not anything youd like her to be\n","Tokenized:  ['ins', '##ta', 'ne', '##va', '##ep', '##i', 'name', 'nevada', 'body', 'neutral', '04', 'brow', 'arched', 'natural', 'black', 'hair', 'two', 'braid', '##s', 'green', 'eyes', 'deepest', 'down', '##turn', 'aqua', 'blue', 'face', 'diamond', 'nose', 'defined', 'natural', 'lips', 'full', 'round', 'po', '##ut', '##y', 'deep', 're', 'matt', '##e', 'outfit', 'put', 'her', 'in', 'something', 'bad', '##ass', 'like', 'a', 'leather', 'jacket', 'role', 'can', 'she', 'be', 'a', 'friend', 'of', 'the', 'mc', 'if', 'not', 'anything', 'you', '##d', 'like', 'her', 'to', 'be']\n","Token IDs:  [16021, 2696, 11265, 3567, 13699, 2072, 2171, 7756, 2303, 8699, 5840, 8306, 9194, 3019, 2304, 2606, 2048, 24148, 2015, 2665, 2159, 17578, 2091, 22299, 28319, 2630, 2227, 6323, 4451, 4225, 3019, 2970, 2440, 2461, 13433, 4904, 2100, 2784, 2128, 4717, 2063, 11018, 2404, 2014, 1999, 2242, 2919, 12054, 2066, 1037, 5898, 6598, 2535, 2064, 2016, 2022, 1037, 2767, 1997, 1996, 11338, 2065, 2025, 2505, 2017, 2094, 2066, 2014, 2000, 2022]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mLK2zaK6rYXV","colab_type":"text"},"source":["The below cell will perform one tokenisation pass of the dataset in order to measure the maximum sentence length."]},{"cell_type":"code","metadata":{"id":"LeetVuq9rXoi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597684256939,"user_tz":420,"elapsed":38863,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"1925d1ec-72d9-4c29-913d-c0261f977507"},"source":["max_len = 0\n","# For first 10 sentences - \n","for s in sentences[:10]:\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(s, truncation=True, add_special_tokens=True)\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Max sentence length:  77\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U65mn_Dgse7t","colab_type":"text"},"source":["\n","\n","For BERT, all sentences must be padded or truncated to a single, fixed length. The maximum sentence length is 512 tokens so you may have to have to split the post_text. The maximum length does impact training and evaluation speed, however. For example, with a Tesla K80 (Colab GPU):\n","\n","`MAX_LEN = 128 --> Training epochs take ~5:28 each`\n","\n","`MAX_LEN = 64 --> Training epochs take ~2:57 each`\n","\n","Try and encode the dataset with the DistilBertTokenizer below.\n","\n","See [docs here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode) for `tokenizer.encode` . \n"]},{"cell_type":"code","metadata":{"id":"ap40wO-51_lw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684256940,"user_tz":420,"elapsed":38850,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["tokenizer.encode?"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhSh0z1ZZIYN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1597684258511,"user_tz":420,"elapsed":40412,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"278f5a49-65f0-474b-fdc2-fe4b62b58b3d"},"source":["# Let's try it on a small subset of our data\n","tokenized = sampled_data['text'].iloc[:1000].apply(lambda x: tokenizer.encode(x, truncation=True, add_special_tokens=True))\n","tokenized.head()"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["431570    [101, 2092, 3849, 5875, 2012, 2560, 2453, 3046...\n","372215    [101, 16021, 2696, 11265, 3567, 13699, 2072, 2...\n","568337    [101, 6854, 2009, 2145, 3849, 2000, 2022, 2004...\n","373236    [101, 1045, 2031, 2589, 2070, 2077, 1045, 2064...\n","77862     [101, 2004, 5498, 2638, 9148, 16150, 5358, 204...\n","Name: text, dtype: object"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"TmyhO_m7m_5H","colab_type":"text"},"source":["### 2.1.2 Padding the Sentences and Attention Mask \n","\n","- For BERT, all sentences must be padded or truncated to a single, fixed length.\n","The maximum sentence length is 512 tokens.\n","- Padding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a “MAX_LEN” of 8 tokens.\n","\n","\n","\n","\n","<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"500\"/>\n","\n","The “Attention Mask” is simply an array of 1s and 0s indicating which tokens are padding and which aren’t (seems kind of redundant, doesn’t it?!). This mask tells the “Self-Attention” mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n","\n","\n","Therefore the encoding task requires the following - \n","\n","1. Split the sentence into tokens.\n","2. Add the special [CLS] and [SEP] tokens.\n","3. Pad or truncate all sentences to the same length.\n","4. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n","5. Map the tokens to their IDs.\n","\n","\n","You should try implement the padding and attention masks yourself with matrix multiplication via numpy. It is trained on lower-cased English text. Hence we set the flag **do_lower_case** to true in BertTokenizer.\n","\n","\n","Otherwise, the first four features are in `tokenizer.encode`, but you can also try use the `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"]},{"cell_type":"code","metadata":{"id":"MQemkj2BqqJ9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1597684553845,"user_tz":420,"elapsed":335731,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"cfd6ac62-285d-4152-87b2-0f65ba8d1b5f"},"source":["import torch\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","token_ids = []\n","attention_masks = []\n","T = 128\n","\n","for s in sentences:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start and append the `[SEP]` token to the end.\n","    #   (3) Pad or truncate the sentence to `max_length`\n","    #   (4) Create attention masks for [PAD] tokens\n","    #   (5) Map tokens to their IDs.\n","\n","    # You can encode_plus as function\n","    encoded_dict = tokenizer.encode_plus(\n","                        s,                              # Sentence to encode.\n","                        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                        truncation=True,                # Pad & truncate all sentences.\n","                        max_length = T,                 # Pad & truncate all sentences.\n","                        pad_to_max_length = True,       # Pad & truncate all sentences.\n","                        return_attention_mask = True,   # Construct attn masks.\n","                        return_tensors = 'pt',          # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.\n","    token_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","token_ids = torch.cat(token_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', token_ids[0])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Original:  Well seems interesting at least might try it out later Though Im not sure which Mesa Im currently at might even be 19 3\n","Token IDs: tensor([  101,  2092,  3849,  5875,  2012,  2560,  2453,  3046,  2009,  2041,\n","         2101,  2295, 10047,  2025,  2469,  2029, 15797, 10047,  2747,  2012,\n","         2453,  2130,  2022,  2539,  1017,   102,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R9aB3l9KtUxo","colab_type":"text"},"source":["## 3.1 Train the Classification Model"]},{"cell_type":"markdown","metadata":{"id":"CQYTFu_qNC5d","colab_type":"text"},"source":["### 3.1.1 DistilBert For Sequence Classification\n","\n","\n","For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","Thankfully, the HuggingFace Pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained DistilBERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","\n","We’ll be using [DistilBertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/distilbert.html#distilbertforsequenceclassification). This is the normal DistilBERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","We then pass the sentence embeddings and features through to the linear regression model to evaluate forum predictions.\n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n","\n","<!-- \n","1. Append the classification  layer to the BERT model\n","\n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n","\n","### [Optional] Grid Search for Parameters\n","We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularisation strength. -->"]},{"cell_type":"code","metadata":{"id":"UBTrCeZ6l1OC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597684553846,"user_tz":420,"elapsed":335714,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"0793343d-a90e-4d8b-ef18-a350c309300c"},"source":["from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(token_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(f'{train_size:>5,} training samples')\n","print(f'{val_size:>5,} validation samples')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["180,000 training samples\n","20,000 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uhtNln7harfO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684553846,"user_tz":420,"elapsed":335704,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8vbP8QmjxZK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1597684553847,"user_tz":420,"elapsed":335695,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"2aed233f-3974-4088-f7fa-c955f4e62b6b"},"source":["sampled_data['forum'].unique()"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Level1Techs Forums', 'Episode Forums',\n","       'The Official Gearbox Software Forums', 'Amazon Seller Forums',\n","       'SmartThings Community', 'Discuss the Elastic Stack',\n","       'Car Talk Community', 'Revolut Community',\n","       'GitHub Support Community', 'Forums - T Nation'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"3c1s0qjqeAj5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597684559490,"user_tz":420,"elapsed":341329,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"83afbf79-c1f5-46f1-fd9e-c56200a637e6"},"source":["from transformers import DistilBertForSequenceClassification, AdamW, BertConfig\n","\n","# Load DistilBertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = DistilBertForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = len(sampled_data['forum'].unique()), # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU. Make sure you enable the runtime clicking [Runtime]->[Change Runtime Type]->[Hardware Accelerator]->GPU->[Save]\n","model.cuda()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=10, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"7dgr40_WPatk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1597684559491,"user_tz":420,"elapsed":341319,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"5bff0628-d4a9-450b-ad12-be0844d90efe"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["The BERT model has 104 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","distilbert.embeddings.word_embeddings.weight            (30522, 768)\n","distilbert.embeddings.position_embeddings.weight          (512, 768)\n","distilbert.embeddings.LayerNorm.weight                        (768,)\n","distilbert.embeddings.LayerNorm.bias                          (768,)\n","distilbert.transformer.layer.0.attention.q_lin.weight     (768, 768)\n","\n","==== First Transformer ====\n","\n","distilbert.transformer.layer.0.attention.q_lin.bias           (768,)\n","distilbert.transformer.layer.0.attention.k_lin.weight     (768, 768)\n","distilbert.transformer.layer.0.attention.k_lin.bias           (768,)\n","distilbert.transformer.layer.0.attention.v_lin.weight     (768, 768)\n","distilbert.transformer.layer.0.attention.v_lin.bias           (768,)\n","distilbert.transformer.layer.0.attention.out_lin.weight   (768, 768)\n","distilbert.transformer.layer.0.attention.out_lin.bias         (768,)\n","distilbert.transformer.layer.0.sa_layer_norm.weight           (768,)\n","distilbert.transformer.layer.0.sa_layer_norm.bias             (768,)\n","distilbert.transformer.layer.0.ffn.lin1.weight           (3072, 768)\n","distilbert.transformer.layer.0.ffn.lin1.bias                 (3072,)\n","distilbert.transformer.layer.0.ffn.lin2.weight           (768, 3072)\n","distilbert.transformer.layer.0.ffn.lin2.bias                  (768,)\n","distilbert.transformer.layer.0.output_layer_norm.weight       (768,)\n","distilbert.transformer.layer.0.output_layer_norm.bias         (768,)\n","distilbert.transformer.layer.1.attention.q_lin.weight     (768, 768)\n","\n","==== Output Layer ====\n","\n","pre_classifier.weight                                     (768, 768)\n","pre_classifier.bias                                           (768,)\n","classifier.weight                                          (10, 768)\n","classifier.bias                                                (10,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v0xZ29SHQtzx","colab_type":"text"},"source":["### 3.1.2 Optimizer & Learning Rate Scheduler\n","\n","Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n","\n","For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper):\n","\n","* Batch size: 16, 32\n","* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n","* Number of epochs: 2, 3, 4\n","\n","We chose:\n","\n","* Batch size: 32 (set when creating our DataLoaders)\n","* Learning rate: 2e-5\n","* Epochs: 4 (we’ll see that this is probably too many…)\n","\n","\n","The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation” (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n","\n","You can find the creation of the AdamW optimizer in run_glue.py [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."]},{"cell_type":"code","metadata":{"id":"6s03rxX0QJbs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684559492,"user_tz":420,"elapsed":341302,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6ciijeVRPPr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684559493,"user_tz":420,"elapsed":341288,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcGtckVDQRih","colab_type":"text"},"source":["Below is our training loop. There’s a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase.\n","\n","**Training:**\n","\n","* Unpack our data inputs and labels\n","* Load data onto the GPU for acceleration\n","* Clear out the gradients calculated in the previous pass.\n","* In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n","* Forward pass (feed input data through the network)\n","* Backward pass (backpropagation)\n","* Tell the network to update parameters with optimizer.step()\n","* Track variables for monitoring progress\n","\n","**Evalution:**\n","* Unpack our data inputs and labels\n","* Load data onto the GPU for acceleration\n","* Forward pass (feed input data through the network)\n","* Compute loss on our validation data and track variables for monitoring progress\n","\n","Pytorch hides all of the detailed calculations from us, but we’ve commented the code to point out which of the above steps are happening on each line.\n","\n","Define a helper function for calculating accuracy."]},{"cell_type":"code","metadata":{"id":"DJoUD6DkQL0J","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684559494,"user_tz":420,"elapsed":341276,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3-MSfTnQOyk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597684559494,"user_tz":420,"elapsed":341266,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["import time\n","import datetime\n","\n","# Helper function for formatting elapsed times as hh:mm:ss\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqG7FzRVFEIv","colab_type":"text"},"source":["In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "]},{"cell_type":"code","metadata":{"id":"oYsV4H8fCpZ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597684559495,"user_tz":420,"elapsed":341254,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"239a3c88-4e95-4f61-a554-0f0ee6e14ef3"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":35,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6J-FYdx6nFE_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597698827786,"user_tz":420,"elapsed":14609528,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"a4b713df-ed11-4ad6-eb93-548ded40f595"},"source":["import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].type(torch.LongTensor).to(device)\n","        # print(b_input_ids.shape)\n","       \n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids,\n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":36,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:24.\n","  Batch    80  of  5,625.    Elapsed: 0:00:47.\n","  Batch   120  of  5,625.    Elapsed: 0:01:11.\n","  Batch   160  of  5,625.    Elapsed: 0:01:36.\n","  Batch   200  of  5,625.    Elapsed: 0:02:00.\n","  Batch   240  of  5,625.    Elapsed: 0:02:24.\n","  Batch   280  of  5,625.    Elapsed: 0:02:48.\n","  Batch   320  of  5,625.    Elapsed: 0:03:12.\n","  Batch   360  of  5,625.    Elapsed: 0:03:36.\n","  Batch   400  of  5,625.    Elapsed: 0:04:00.\n","  Batch   440  of  5,625.    Elapsed: 0:04:24.\n","  Batch   480  of  5,625.    Elapsed: 0:04:49.\n","  Batch   520  of  5,625.    Elapsed: 0:05:13.\n","  Batch   560  of  5,625.    Elapsed: 0:05:37.\n","  Batch   600  of  5,625.    Elapsed: 0:06:01.\n","  Batch   640  of  5,625.    Elapsed: 0:06:25.\n","  Batch   680  of  5,625.    Elapsed: 0:06:49.\n","  Batch   720  of  5,625.    Elapsed: 0:07:13.\n","  Batch   760  of  5,625.    Elapsed: 0:07:37.\n","  Batch   800  of  5,625.    Elapsed: 0:08:01.\n","  Batch   840  of  5,625.    Elapsed: 0:08:25.\n","  Batch   880  of  5,625.    Elapsed: 0:08:50.\n","  Batch   920  of  5,625.    Elapsed: 0:09:14.\n","  Batch   960  of  5,625.    Elapsed: 0:09:38.\n","  Batch 1,000  of  5,625.    Elapsed: 0:10:02.\n","  Batch 1,040  of  5,625.    Elapsed: 0:10:26.\n","  Batch 1,080  of  5,625.    Elapsed: 0:10:50.\n","  Batch 1,120  of  5,625.    Elapsed: 0:11:14.\n","  Batch 1,160  of  5,625.    Elapsed: 0:11:38.\n","  Batch 1,200  of  5,625.    Elapsed: 0:12:03.\n","  Batch 1,240  of  5,625.    Elapsed: 0:12:27.\n","  Batch 1,280  of  5,625.    Elapsed: 0:12:51.\n","  Batch 1,320  of  5,625.    Elapsed: 0:13:15.\n","  Batch 1,360  of  5,625.    Elapsed: 0:13:39.\n","  Batch 1,400  of  5,625.    Elapsed: 0:14:03.\n","  Batch 1,440  of  5,625.    Elapsed: 0:14:27.\n","  Batch 1,480  of  5,625.    Elapsed: 0:14:51.\n","  Batch 1,520  of  5,625.    Elapsed: 0:15:15.\n","  Batch 1,560  of  5,625.    Elapsed: 0:15:40.\n","  Batch 1,600  of  5,625.    Elapsed: 0:16:04.\n","  Batch 1,640  of  5,625.    Elapsed: 0:16:28.\n","  Batch 1,680  of  5,625.    Elapsed: 0:16:52.\n","  Batch 1,720  of  5,625.    Elapsed: 0:17:16.\n","  Batch 1,760  of  5,625.    Elapsed: 0:17:40.\n","  Batch 1,800  of  5,625.    Elapsed: 0:18:04.\n","  Batch 1,840  of  5,625.    Elapsed: 0:18:28.\n","  Batch 1,880  of  5,625.    Elapsed: 0:18:52.\n","  Batch 1,920  of  5,625.    Elapsed: 0:19:17.\n","  Batch 1,960  of  5,625.    Elapsed: 0:19:41.\n","  Batch 2,000  of  5,625.    Elapsed: 0:20:05.\n","  Batch 2,040  of  5,625.    Elapsed: 0:20:29.\n","  Batch 2,080  of  5,625.    Elapsed: 0:20:53.\n","  Batch 2,120  of  5,625.    Elapsed: 0:21:17.\n","  Batch 2,160  of  5,625.    Elapsed: 0:21:41.\n","  Batch 2,200  of  5,625.    Elapsed: 0:22:05.\n","  Batch 2,240  of  5,625.    Elapsed: 0:22:29.\n","  Batch 2,280  of  5,625.    Elapsed: 0:22:53.\n","  Batch 2,320  of  5,625.    Elapsed: 0:23:17.\n","  Batch 2,360  of  5,625.    Elapsed: 0:23:41.\n","  Batch 2,400  of  5,625.    Elapsed: 0:24:05.\n","  Batch 2,440  of  5,625.    Elapsed: 0:24:29.\n","  Batch 2,480  of  5,625.    Elapsed: 0:24:54.\n","  Batch 2,520  of  5,625.    Elapsed: 0:25:18.\n","  Batch 2,560  of  5,625.    Elapsed: 0:25:42.\n","  Batch 2,600  of  5,625.    Elapsed: 0:26:06.\n","  Batch 2,640  of  5,625.    Elapsed: 0:26:30.\n","  Batch 2,680  of  5,625.    Elapsed: 0:26:54.\n","  Batch 2,720  of  5,625.    Elapsed: 0:27:18.\n","  Batch 2,760  of  5,625.    Elapsed: 0:27:42.\n","  Batch 2,800  of  5,625.    Elapsed: 0:28:06.\n","  Batch 2,840  of  5,625.    Elapsed: 0:28:31.\n","  Batch 2,880  of  5,625.    Elapsed: 0:28:55.\n","  Batch 2,920  of  5,625.    Elapsed: 0:29:19.\n","  Batch 2,960  of  5,625.    Elapsed: 0:29:43.\n","  Batch 3,000  of  5,625.    Elapsed: 0:30:07.\n","  Batch 3,040  of  5,625.    Elapsed: 0:30:31.\n","  Batch 3,080  of  5,625.    Elapsed: 0:30:55.\n","  Batch 3,120  of  5,625.    Elapsed: 0:31:19.\n","  Batch 3,160  of  5,625.    Elapsed: 0:31:43.\n","  Batch 3,200  of  5,625.    Elapsed: 0:32:08.\n","  Batch 3,240  of  5,625.    Elapsed: 0:32:32.\n","  Batch 3,280  of  5,625.    Elapsed: 0:32:56.\n","  Batch 3,320  of  5,625.    Elapsed: 0:33:20.\n","  Batch 3,360  of  5,625.    Elapsed: 0:33:44.\n","  Batch 3,400  of  5,625.    Elapsed: 0:34:09.\n","  Batch 3,440  of  5,625.    Elapsed: 0:34:33.\n","  Batch 3,480  of  5,625.    Elapsed: 0:34:57.\n","  Batch 3,520  of  5,625.    Elapsed: 0:35:21.\n","  Batch 3,560  of  5,625.    Elapsed: 0:35:45.\n","  Batch 3,600  of  5,625.    Elapsed: 0:36:09.\n","  Batch 3,640  of  5,625.    Elapsed: 0:36:33.\n","  Batch 3,680  of  5,625.    Elapsed: 0:36:57.\n","  Batch 3,720  of  5,625.    Elapsed: 0:37:21.\n","  Batch 3,760  of  5,625.    Elapsed: 0:37:45.\n","  Batch 3,800  of  5,625.    Elapsed: 0:38:10.\n","  Batch 3,840  of  5,625.    Elapsed: 0:38:34.\n","  Batch 3,880  of  5,625.    Elapsed: 0:38:58.\n","  Batch 3,920  of  5,625.    Elapsed: 0:39:22.\n","  Batch 3,960  of  5,625.    Elapsed: 0:39:47.\n","  Batch 4,000  of  5,625.    Elapsed: 0:40:11.\n","  Batch 4,040  of  5,625.    Elapsed: 0:40:36.\n","  Batch 4,080  of  5,625.    Elapsed: 0:41:01.\n","  Batch 4,120  of  5,625.    Elapsed: 0:41:24.\n","  Batch 4,160  of  5,625.    Elapsed: 0:41:48.\n","  Batch 4,200  of  5,625.    Elapsed: 0:42:12.\n","  Batch 4,240  of  5,625.    Elapsed: 0:42:37.\n","  Batch 4,280  of  5,625.    Elapsed: 0:43:01.\n","  Batch 4,320  of  5,625.    Elapsed: 0:43:26.\n","  Batch 4,360  of  5,625.    Elapsed: 0:43:50.\n","  Batch 4,400  of  5,625.    Elapsed: 0:44:15.\n","  Batch 4,440  of  5,625.    Elapsed: 0:44:39.\n","  Batch 4,480  of  5,625.    Elapsed: 0:45:03.\n","  Batch 4,520  of  5,625.    Elapsed: 0:45:28.\n","  Batch 4,560  of  5,625.    Elapsed: 0:45:52.\n","  Batch 4,600  of  5,625.    Elapsed: 0:46:17.\n","  Batch 4,640  of  5,625.    Elapsed: 0:46:41.\n","  Batch 4,680  of  5,625.    Elapsed: 0:47:06.\n","  Batch 4,720  of  5,625.    Elapsed: 0:47:30.\n","  Batch 4,760  of  5,625.    Elapsed: 0:47:55.\n","  Batch 4,800  of  5,625.    Elapsed: 0:48:19.\n","  Batch 4,840  of  5,625.    Elapsed: 0:48:44.\n","  Batch 4,880  of  5,625.    Elapsed: 0:49:08.\n","  Batch 4,920  of  5,625.    Elapsed: 0:49:33.\n","  Batch 4,960  of  5,625.    Elapsed: 0:49:57.\n","  Batch 5,000  of  5,625.    Elapsed: 0:50:22.\n","  Batch 5,040  of  5,625.    Elapsed: 0:50:46.\n","  Batch 5,080  of  5,625.    Elapsed: 0:51:11.\n","  Batch 5,120  of  5,625.    Elapsed: 0:51:35.\n","  Batch 5,160  of  5,625.    Elapsed: 0:52:00.\n","  Batch 5,200  of  5,625.    Elapsed: 0:52:24.\n","  Batch 5,240  of  5,625.    Elapsed: 0:52:49.\n","  Batch 5,280  of  5,625.    Elapsed: 0:53:13.\n","  Batch 5,320  of  5,625.    Elapsed: 0:53:38.\n","  Batch 5,360  of  5,625.    Elapsed: 0:54:02.\n","  Batch 5,400  of  5,625.    Elapsed: 0:54:27.\n","  Batch 5,440  of  5,625.    Elapsed: 0:54:51.\n","  Batch 5,480  of  5,625.    Elapsed: 0:55:16.\n","  Batch 5,520  of  5,625.    Elapsed: 0:55:40.\n","  Batch 5,560  of  5,625.    Elapsed: 0:56:05.\n","  Batch 5,600  of  5,625.    Elapsed: 0:56:29.\n","\n","  Average training loss: 0.54\n","  Training epcoh took: 0:56:44\n","\n","Running Validation...\n","  Accuracy: 0.88\n","  Validation took: 0:02:15\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:25.\n","  Batch    80  of  5,625.    Elapsed: 0:00:49.\n","  Batch   120  of  5,625.    Elapsed: 0:01:14.\n","  Batch   160  of  5,625.    Elapsed: 0:01:38.\n","  Batch   200  of  5,625.    Elapsed: 0:02:02.\n","  Batch   240  of  5,625.    Elapsed: 0:02:27.\n","  Batch   280  of  5,625.    Elapsed: 0:02:51.\n","  Batch   320  of  5,625.    Elapsed: 0:03:16.\n","  Batch   360  of  5,625.    Elapsed: 0:03:40.\n","  Batch   400  of  5,625.    Elapsed: 0:04:05.\n","  Batch   440  of  5,625.    Elapsed: 0:04:29.\n","  Batch   480  of  5,625.    Elapsed: 0:04:54.\n","  Batch   520  of  5,625.    Elapsed: 0:05:18.\n","  Batch   560  of  5,625.    Elapsed: 0:05:43.\n","  Batch   600  of  5,625.    Elapsed: 0:06:07.\n","  Batch   640  of  5,625.    Elapsed: 0:06:32.\n","  Batch   680  of  5,625.    Elapsed: 0:06:56.\n","  Batch   720  of  5,625.    Elapsed: 0:07:21.\n","  Batch   760  of  5,625.    Elapsed: 0:07:45.\n","  Batch   800  of  5,625.    Elapsed: 0:08:10.\n","  Batch   840  of  5,625.    Elapsed: 0:08:34.\n","  Batch   880  of  5,625.    Elapsed: 0:08:59.\n","  Batch   920  of  5,625.    Elapsed: 0:09:23.\n","  Batch   960  of  5,625.    Elapsed: 0:09:48.\n","  Batch 1,000  of  5,625.    Elapsed: 0:10:12.\n","  Batch 1,040  of  5,625.    Elapsed: 0:10:37.\n","  Batch 1,080  of  5,625.    Elapsed: 0:11:01.\n","  Batch 1,120  of  5,625.    Elapsed: 0:11:26.\n","  Batch 1,160  of  5,625.    Elapsed: 0:11:50.\n","  Batch 1,200  of  5,625.    Elapsed: 0:12:15.\n","  Batch 1,240  of  5,625.    Elapsed: 0:12:39.\n","  Batch 1,280  of  5,625.    Elapsed: 0:13:04.\n","  Batch 1,320  of  5,625.    Elapsed: 0:13:28.\n","  Batch 1,360  of  5,625.    Elapsed: 0:13:53.\n","  Batch 1,400  of  5,625.    Elapsed: 0:14:17.\n","  Batch 1,440  of  5,625.    Elapsed: 0:14:42.\n","  Batch 1,480  of  5,625.    Elapsed: 0:15:06.\n","  Batch 1,520  of  5,625.    Elapsed: 0:15:31.\n","  Batch 1,560  of  5,625.    Elapsed: 0:15:55.\n","  Batch 1,600  of  5,625.    Elapsed: 0:16:20.\n","  Batch 1,640  of  5,625.    Elapsed: 0:16:44.\n","  Batch 1,680  of  5,625.    Elapsed: 0:17:09.\n","  Batch 1,720  of  5,625.    Elapsed: 0:17:33.\n","  Batch 1,760  of  5,625.    Elapsed: 0:17:58.\n","  Batch 1,800  of  5,625.    Elapsed: 0:18:22.\n","  Batch 1,840  of  5,625.    Elapsed: 0:18:47.\n","  Batch 1,880  of  5,625.    Elapsed: 0:19:11.\n","  Batch 1,920  of  5,625.    Elapsed: 0:19:36.\n","  Batch 1,960  of  5,625.    Elapsed: 0:20:00.\n","  Batch 2,000  of  5,625.    Elapsed: 0:20:25.\n","  Batch 2,040  of  5,625.    Elapsed: 0:20:49.\n","  Batch 2,080  of  5,625.    Elapsed: 0:21:13.\n","  Batch 2,120  of  5,625.    Elapsed: 0:21:38.\n","  Batch 2,160  of  5,625.    Elapsed: 0:22:02.\n","  Batch 2,200  of  5,625.    Elapsed: 0:22:27.\n","  Batch 2,240  of  5,625.    Elapsed: 0:22:51.\n","  Batch 2,280  of  5,625.    Elapsed: 0:23:16.\n","  Batch 2,320  of  5,625.    Elapsed: 0:23:40.\n","  Batch 2,360  of  5,625.    Elapsed: 0:24:05.\n","  Batch 2,400  of  5,625.    Elapsed: 0:24:29.\n","  Batch 2,440  of  5,625.    Elapsed: 0:24:54.\n","  Batch 2,480  of  5,625.    Elapsed: 0:25:18.\n","  Batch 2,520  of  5,625.    Elapsed: 0:25:43.\n","  Batch 2,560  of  5,625.    Elapsed: 0:26:07.\n","  Batch 2,600  of  5,625.    Elapsed: 0:26:32.\n","  Batch 2,640  of  5,625.    Elapsed: 0:26:56.\n","  Batch 2,680  of  5,625.    Elapsed: 0:27:21.\n","  Batch 2,720  of  5,625.    Elapsed: 0:27:45.\n","  Batch 2,760  of  5,625.    Elapsed: 0:28:09.\n","  Batch 2,800  of  5,625.    Elapsed: 0:28:34.\n","  Batch 2,840  of  5,625.    Elapsed: 0:28:58.\n","  Batch 2,880  of  5,625.    Elapsed: 0:29:22.\n","  Batch 2,920  of  5,625.    Elapsed: 0:29:47.\n","  Batch 2,960  of  5,625.    Elapsed: 0:30:11.\n","  Batch 3,000  of  5,625.    Elapsed: 0:30:36.\n","  Batch 3,040  of  5,625.    Elapsed: 0:31:00.\n","  Batch 3,080  of  5,625.    Elapsed: 0:31:25.\n","  Batch 3,120  of  5,625.    Elapsed: 0:31:49.\n","  Batch 3,160  of  5,625.    Elapsed: 0:32:14.\n","  Batch 3,200  of  5,625.    Elapsed: 0:32:38.\n","  Batch 3,240  of  5,625.    Elapsed: 0:33:03.\n","  Batch 3,280  of  5,625.    Elapsed: 0:33:27.\n","  Batch 3,320  of  5,625.    Elapsed: 0:33:52.\n","  Batch 3,360  of  5,625.    Elapsed: 0:34:16.\n","  Batch 3,400  of  5,625.    Elapsed: 0:34:41.\n","  Batch 3,440  of  5,625.    Elapsed: 0:35:05.\n","  Batch 3,480  of  5,625.    Elapsed: 0:35:29.\n","  Batch 3,520  of  5,625.    Elapsed: 0:35:54.\n","  Batch 3,560  of  5,625.    Elapsed: 0:36:18.\n","  Batch 3,600  of  5,625.    Elapsed: 0:36:43.\n","  Batch 3,640  of  5,625.    Elapsed: 0:37:07.\n","  Batch 3,680  of  5,625.    Elapsed: 0:37:32.\n","  Batch 3,720  of  5,625.    Elapsed: 0:37:56.\n","  Batch 3,760  of  5,625.    Elapsed: 0:38:21.\n","  Batch 3,800  of  5,625.    Elapsed: 0:38:45.\n","  Batch 3,840  of  5,625.    Elapsed: 0:39:10.\n","  Batch 3,880  of  5,625.    Elapsed: 0:39:34.\n","  Batch 3,920  of  5,625.    Elapsed: 0:39:59.\n","  Batch 3,960  of  5,625.    Elapsed: 0:40:23.\n","  Batch 4,000  of  5,625.    Elapsed: 0:40:48.\n","  Batch 4,040  of  5,625.    Elapsed: 0:41:12.\n","  Batch 4,080  of  5,625.    Elapsed: 0:41:37.\n","  Batch 4,120  of  5,625.    Elapsed: 0:42:01.\n","  Batch 4,160  of  5,625.    Elapsed: 0:42:25.\n","  Batch 4,200  of  5,625.    Elapsed: 0:42:50.\n","  Batch 4,240  of  5,625.    Elapsed: 0:43:14.\n","  Batch 4,280  of  5,625.    Elapsed: 0:43:39.\n","  Batch 4,320  of  5,625.    Elapsed: 0:44:03.\n","  Batch 4,360  of  5,625.    Elapsed: 0:44:28.\n","  Batch 4,400  of  5,625.    Elapsed: 0:44:52.\n","  Batch 4,440  of  5,625.    Elapsed: 0:45:17.\n","  Batch 4,480  of  5,625.    Elapsed: 0:45:41.\n","  Batch 4,520  of  5,625.    Elapsed: 0:46:06.\n","  Batch 4,560  of  5,625.    Elapsed: 0:46:30.\n","  Batch 4,600  of  5,625.    Elapsed: 0:46:54.\n","  Batch 4,640  of  5,625.    Elapsed: 0:47:19.\n","  Batch 4,680  of  5,625.    Elapsed: 0:47:43.\n","  Batch 4,720  of  5,625.    Elapsed: 0:48:08.\n","  Batch 4,760  of  5,625.    Elapsed: 0:48:33.\n","  Batch 4,800  of  5,625.    Elapsed: 0:48:57.\n","  Batch 4,840  of  5,625.    Elapsed: 0:49:21.\n","  Batch 4,880  of  5,625.    Elapsed: 0:49:46.\n","  Batch 4,920  of  5,625.    Elapsed: 0:50:10.\n","  Batch 4,960  of  5,625.    Elapsed: 0:50:35.\n","  Batch 5,000  of  5,625.    Elapsed: 0:50:59.\n","  Batch 5,040  of  5,625.    Elapsed: 0:51:24.\n","  Batch 5,080  of  5,625.    Elapsed: 0:51:48.\n","  Batch 5,120  of  5,625.    Elapsed: 0:52:13.\n","  Batch 5,160  of  5,625.    Elapsed: 0:52:37.\n","  Batch 5,200  of  5,625.    Elapsed: 0:53:02.\n","  Batch 5,240  of  5,625.    Elapsed: 0:53:26.\n","  Batch 5,280  of  5,625.    Elapsed: 0:53:51.\n","  Batch 5,320  of  5,625.    Elapsed: 0:54:15.\n","  Batch 5,360  of  5,625.    Elapsed: 0:54:40.\n","  Batch 5,400  of  5,625.    Elapsed: 0:55:04.\n","  Batch 5,440  of  5,625.    Elapsed: 0:55:29.\n","  Batch 5,480  of  5,625.    Elapsed: 0:55:53.\n","  Batch 5,520  of  5,625.    Elapsed: 0:56:17.\n","  Batch 5,560  of  5,625.    Elapsed: 0:56:42.\n","  Batch 5,600  of  5,625.    Elapsed: 0:57:06.\n","\n","  Average training loss: 0.31\n","  Training epcoh took: 0:57:22\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  Validation took: 0:02:14\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:24.\n","  Batch    80  of  5,625.    Elapsed: 0:00:49.\n","  Batch   120  of  5,625.    Elapsed: 0:01:13.\n","  Batch   160  of  5,625.    Elapsed: 0:01:38.\n","  Batch   200  of  5,625.    Elapsed: 0:02:02.\n","  Batch   240  of  5,625.    Elapsed: 0:02:27.\n","  Batch   280  of  5,625.    Elapsed: 0:02:51.\n","  Batch   320  of  5,625.    Elapsed: 0:03:16.\n","  Batch   360  of  5,625.    Elapsed: 0:03:40.\n","  Batch   400  of  5,625.    Elapsed: 0:04:05.\n","  Batch   440  of  5,625.    Elapsed: 0:04:29.\n","  Batch   480  of  5,625.    Elapsed: 0:04:54.\n","  Batch   520  of  5,625.    Elapsed: 0:05:18.\n","  Batch   560  of  5,625.    Elapsed: 0:05:43.\n","  Batch   600  of  5,625.    Elapsed: 0:06:07.\n","  Batch   640  of  5,625.    Elapsed: 0:06:32.\n","  Batch   680  of  5,625.    Elapsed: 0:06:56.\n","  Batch   720  of  5,625.    Elapsed: 0:07:21.\n","  Batch   760  of  5,625.    Elapsed: 0:07:45.\n","  Batch   800  of  5,625.    Elapsed: 0:08:09.\n","  Batch   840  of  5,625.    Elapsed: 0:08:34.\n","  Batch   880  of  5,625.    Elapsed: 0:08:58.\n","  Batch   920  of  5,625.    Elapsed: 0:09:23.\n","  Batch   960  of  5,625.    Elapsed: 0:09:47.\n","  Batch 1,000  of  5,625.    Elapsed: 0:10:12.\n","  Batch 1,040  of  5,625.    Elapsed: 0:10:36.\n","  Batch 1,080  of  5,625.    Elapsed: 0:11:01.\n","  Batch 1,120  of  5,625.    Elapsed: 0:11:25.\n","  Batch 1,160  of  5,625.    Elapsed: 0:11:50.\n","  Batch 1,200  of  5,625.    Elapsed: 0:12:14.\n","  Batch 1,240  of  5,625.    Elapsed: 0:12:39.\n","  Batch 1,280  of  5,625.    Elapsed: 0:13:03.\n","  Batch 1,320  of  5,625.    Elapsed: 0:13:28.\n","  Batch 1,360  of  5,625.    Elapsed: 0:13:52.\n","  Batch 1,400  of  5,625.    Elapsed: 0:14:17.\n","  Batch 1,440  of  5,625.    Elapsed: 0:14:41.\n","  Batch 1,480  of  5,625.    Elapsed: 0:15:05.\n","  Batch 1,520  of  5,625.    Elapsed: 0:15:30.\n","  Batch 1,560  of  5,625.    Elapsed: 0:15:54.\n","  Batch 1,600  of  5,625.    Elapsed: 0:16:19.\n","  Batch 1,640  of  5,625.    Elapsed: 0:16:43.\n","  Batch 1,680  of  5,625.    Elapsed: 0:17:08.\n","  Batch 1,720  of  5,625.    Elapsed: 0:17:32.\n","  Batch 1,760  of  5,625.    Elapsed: 0:17:57.\n","  Batch 1,800  of  5,625.    Elapsed: 0:18:21.\n","  Batch 1,840  of  5,625.    Elapsed: 0:18:46.\n","  Batch 1,880  of  5,625.    Elapsed: 0:19:10.\n","  Batch 1,920  of  5,625.    Elapsed: 0:19:35.\n","  Batch 1,960  of  5,625.    Elapsed: 0:19:59.\n","  Batch 2,000  of  5,625.    Elapsed: 0:20:24.\n","  Batch 2,040  of  5,625.    Elapsed: 0:20:48.\n","  Batch 2,080  of  5,625.    Elapsed: 0:21:13.\n","  Batch 2,120  of  5,625.    Elapsed: 0:21:37.\n","  Batch 2,160  of  5,625.    Elapsed: 0:22:02.\n","  Batch 2,200  of  5,625.    Elapsed: 0:22:26.\n","  Batch 2,240  of  5,625.    Elapsed: 0:22:50.\n","  Batch 2,280  of  5,625.    Elapsed: 0:23:15.\n","  Batch 2,320  of  5,625.    Elapsed: 0:23:39.\n","  Batch 2,360  of  5,625.    Elapsed: 0:24:04.\n","  Batch 2,400  of  5,625.    Elapsed: 0:24:28.\n","  Batch 2,440  of  5,625.    Elapsed: 0:24:53.\n","  Batch 2,480  of  5,625.    Elapsed: 0:25:17.\n","  Batch 2,520  of  5,625.    Elapsed: 0:25:42.\n","  Batch 2,560  of  5,625.    Elapsed: 0:26:06.\n","  Batch 2,600  of  5,625.    Elapsed: 0:26:31.\n","  Batch 2,640  of  5,625.    Elapsed: 0:26:55.\n","  Batch 2,680  of  5,625.    Elapsed: 0:27:19.\n","  Batch 2,720  of  5,625.    Elapsed: 0:27:44.\n","  Batch 2,760  of  5,625.    Elapsed: 0:28:08.\n","  Batch 2,800  of  5,625.    Elapsed: 0:28:33.\n","  Batch 2,840  of  5,625.    Elapsed: 0:28:57.\n","  Batch 2,880  of  5,625.    Elapsed: 0:29:22.\n","  Batch 2,920  of  5,625.    Elapsed: 0:29:46.\n","  Batch 2,960  of  5,625.    Elapsed: 0:30:11.\n","  Batch 3,000  of  5,625.    Elapsed: 0:30:35.\n","  Batch 3,040  of  5,625.    Elapsed: 0:31:00.\n","  Batch 3,080  of  5,625.    Elapsed: 0:31:24.\n","  Batch 3,120  of  5,625.    Elapsed: 0:31:49.\n","  Batch 3,160  of  5,625.    Elapsed: 0:32:13.\n","  Batch 3,200  of  5,625.    Elapsed: 0:32:38.\n","  Batch 3,240  of  5,625.    Elapsed: 0:33:02.\n","  Batch 3,280  of  5,625.    Elapsed: 0:33:27.\n","  Batch 3,320  of  5,625.    Elapsed: 0:33:51.\n","  Batch 3,360  of  5,625.    Elapsed: 0:34:16.\n","  Batch 3,400  of  5,625.    Elapsed: 0:34:40.\n","  Batch 3,440  of  5,625.    Elapsed: 0:35:04.\n","  Batch 3,480  of  5,625.    Elapsed: 0:35:29.\n","  Batch 3,520  of  5,625.    Elapsed: 0:35:53.\n","  Batch 3,560  of  5,625.    Elapsed: 0:36:18.\n","  Batch 3,600  of  5,625.    Elapsed: 0:36:42.\n","  Batch 3,640  of  5,625.    Elapsed: 0:37:07.\n","  Batch 3,680  of  5,625.    Elapsed: 0:37:31.\n","  Batch 3,720  of  5,625.    Elapsed: 0:37:56.\n","  Batch 3,760  of  5,625.    Elapsed: 0:38:20.\n","  Batch 3,800  of  5,625.    Elapsed: 0:38:45.\n","  Batch 3,840  of  5,625.    Elapsed: 0:39:09.\n","  Batch 3,880  of  5,625.    Elapsed: 0:39:34.\n","  Batch 3,920  of  5,625.    Elapsed: 0:39:58.\n","  Batch 3,960  of  5,625.    Elapsed: 0:40:23.\n","  Batch 4,000  of  5,625.    Elapsed: 0:40:47.\n","  Batch 4,040  of  5,625.    Elapsed: 0:41:12.\n","  Batch 4,080  of  5,625.    Elapsed: 0:41:36.\n","  Batch 4,120  of  5,625.    Elapsed: 0:42:01.\n","  Batch 4,160  of  5,625.    Elapsed: 0:42:25.\n","  Batch 4,200  of  5,625.    Elapsed: 0:42:50.\n","  Batch 4,240  of  5,625.    Elapsed: 0:43:14.\n","  Batch 4,280  of  5,625.    Elapsed: 0:43:39.\n","  Batch 4,320  of  5,625.    Elapsed: 0:44:03.\n","  Batch 4,360  of  5,625.    Elapsed: 0:44:28.\n","  Batch 4,400  of  5,625.    Elapsed: 0:44:52.\n","  Batch 4,440  of  5,625.    Elapsed: 0:45:17.\n","  Batch 4,480  of  5,625.    Elapsed: 0:45:41.\n","  Batch 4,520  of  5,625.    Elapsed: 0:46:06.\n","  Batch 4,560  of  5,625.    Elapsed: 0:46:30.\n","  Batch 4,600  of  5,625.    Elapsed: 0:46:55.\n","  Batch 4,640  of  5,625.    Elapsed: 0:47:19.\n","  Batch 4,680  of  5,625.    Elapsed: 0:47:44.\n","  Batch 4,720  of  5,625.    Elapsed: 0:48:08.\n","  Batch 4,760  of  5,625.    Elapsed: 0:48:32.\n","  Batch 4,800  of  5,625.    Elapsed: 0:48:57.\n","  Batch 4,840  of  5,625.    Elapsed: 0:49:21.\n","  Batch 4,880  of  5,625.    Elapsed: 0:49:46.\n","  Batch 4,920  of  5,625.    Elapsed: 0:50:10.\n","  Batch 4,960  of  5,625.    Elapsed: 0:50:35.\n","  Batch 5,000  of  5,625.    Elapsed: 0:50:59.\n","  Batch 5,040  of  5,625.    Elapsed: 0:51:24.\n","  Batch 5,080  of  5,625.    Elapsed: 0:51:48.\n","  Batch 5,120  of  5,625.    Elapsed: 0:52:13.\n","  Batch 5,160  of  5,625.    Elapsed: 0:52:37.\n","  Batch 5,200  of  5,625.    Elapsed: 0:53:02.\n","  Batch 5,240  of  5,625.    Elapsed: 0:53:26.\n","  Batch 5,280  of  5,625.    Elapsed: 0:53:51.\n","  Batch 5,320  of  5,625.    Elapsed: 0:54:15.\n","  Batch 5,360  of  5,625.    Elapsed: 0:54:40.\n","  Batch 5,400  of  5,625.    Elapsed: 0:55:04.\n","  Batch 5,440  of  5,625.    Elapsed: 0:55:29.\n","  Batch 5,480  of  5,625.    Elapsed: 0:55:53.\n","  Batch 5,520  of  5,625.    Elapsed: 0:56:18.\n","  Batch 5,560  of  5,625.    Elapsed: 0:56:42.\n","  Batch 5,600  of  5,625.    Elapsed: 0:57:07.\n","\n","  Average training loss: 0.22\n","  Training epcoh took: 0:57:22\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:14\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:25.\n","  Batch    80  of  5,625.    Elapsed: 0:00:49.\n","  Batch   120  of  5,625.    Elapsed: 0:01:14.\n","  Batch   160  of  5,625.    Elapsed: 0:01:38.\n","  Batch   200  of  5,625.    Elapsed: 0:02:03.\n","  Batch   240  of  5,625.    Elapsed: 0:02:27.\n","  Batch   280  of  5,625.    Elapsed: 0:02:51.\n","  Batch   320  of  5,625.    Elapsed: 0:03:16.\n","  Batch   360  of  5,625.    Elapsed: 0:03:41.\n","  Batch   400  of  5,625.    Elapsed: 0:04:05.\n","  Batch   440  of  5,625.    Elapsed: 0:04:29.\n","  Batch   480  of  5,625.    Elapsed: 0:04:54.\n","  Batch   520  of  5,625.    Elapsed: 0:05:18.\n","  Batch   560  of  5,625.    Elapsed: 0:05:43.\n","  Batch   600  of  5,625.    Elapsed: 0:06:07.\n","  Batch   640  of  5,625.    Elapsed: 0:06:32.\n","  Batch   680  of  5,625.    Elapsed: 0:06:56.\n","  Batch   720  of  5,625.    Elapsed: 0:07:21.\n","  Batch   760  of  5,625.    Elapsed: 0:07:45.\n","  Batch   800  of  5,625.    Elapsed: 0:08:10.\n","  Batch   840  of  5,625.    Elapsed: 0:08:34.\n","  Batch   880  of  5,625.    Elapsed: 0:08:59.\n","  Batch   920  of  5,625.    Elapsed: 0:09:23.\n","  Batch   960  of  5,625.    Elapsed: 0:09:48.\n","  Batch 1,000  of  5,625.    Elapsed: 0:10:12.\n","  Batch 1,040  of  5,625.    Elapsed: 0:10:37.\n","  Batch 1,080  of  5,625.    Elapsed: 0:11:01.\n","  Batch 1,120  of  5,625.    Elapsed: 0:11:26.\n","  Batch 1,160  of  5,625.    Elapsed: 0:11:50.\n","  Batch 1,200  of  5,625.    Elapsed: 0:12:15.\n","  Batch 1,240  of  5,625.    Elapsed: 0:12:39.\n","  Batch 1,280  of  5,625.    Elapsed: 0:13:04.\n","  Batch 1,320  of  5,625.    Elapsed: 0:13:28.\n","  Batch 1,360  of  5,625.    Elapsed: 0:13:53.\n","  Batch 1,400  of  5,625.    Elapsed: 0:14:17.\n","  Batch 1,440  of  5,625.    Elapsed: 0:14:42.\n","  Batch 1,480  of  5,625.    Elapsed: 0:15:06.\n","  Batch 1,520  of  5,625.    Elapsed: 0:15:31.\n","  Batch 1,560  of  5,625.    Elapsed: 0:15:55.\n","  Batch 1,600  of  5,625.    Elapsed: 0:16:20.\n","  Batch 1,640  of  5,625.    Elapsed: 0:16:44.\n","  Batch 1,680  of  5,625.    Elapsed: 0:17:09.\n","  Batch 1,720  of  5,625.    Elapsed: 0:17:33.\n","  Batch 1,760  of  5,625.    Elapsed: 0:17:58.\n","  Batch 1,800  of  5,625.    Elapsed: 0:18:22.\n","  Batch 1,840  of  5,625.    Elapsed: 0:18:47.\n","  Batch 1,880  of  5,625.    Elapsed: 0:19:11.\n","  Batch 1,920  of  5,625.    Elapsed: 0:19:36.\n","  Batch 1,960  of  5,625.    Elapsed: 0:20:00.\n","  Batch 2,000  of  5,625.    Elapsed: 0:20:25.\n","  Batch 2,040  of  5,625.    Elapsed: 0:20:49.\n","  Batch 2,080  of  5,625.    Elapsed: 0:21:14.\n","  Batch 2,120  of  5,625.    Elapsed: 0:21:38.\n","  Batch 2,160  of  5,625.    Elapsed: 0:22:03.\n","  Batch 2,200  of  5,625.    Elapsed: 0:22:27.\n","  Batch 2,240  of  5,625.    Elapsed: 0:22:52.\n","  Batch 2,280  of  5,625.    Elapsed: 0:23:16.\n","  Batch 2,320  of  5,625.    Elapsed: 0:23:40.\n","  Batch 2,360  of  5,625.    Elapsed: 0:24:05.\n","  Batch 2,400  of  5,625.    Elapsed: 0:24:29.\n","  Batch 2,440  of  5,625.    Elapsed: 0:24:54.\n","  Batch 2,480  of  5,625.    Elapsed: 0:25:18.\n","  Batch 2,520  of  5,625.    Elapsed: 0:25:43.\n","  Batch 2,560  of  5,625.    Elapsed: 0:26:07.\n","  Batch 2,600  of  5,625.    Elapsed: 0:26:32.\n","  Batch 2,640  of  5,625.    Elapsed: 0:26:56.\n","  Batch 2,680  of  5,625.    Elapsed: 0:27:21.\n","  Batch 2,720  of  5,625.    Elapsed: 0:27:45.\n","  Batch 2,760  of  5,625.    Elapsed: 0:28:10.\n","  Batch 2,800  of  5,625.    Elapsed: 0:28:34.\n","  Batch 2,840  of  5,625.    Elapsed: 0:28:59.\n","  Batch 2,880  of  5,625.    Elapsed: 0:29:23.\n","  Batch 2,920  of  5,625.    Elapsed: 0:29:48.\n","  Batch 2,960  of  5,625.    Elapsed: 0:30:12.\n","  Batch 3,000  of  5,625.    Elapsed: 0:30:37.\n","  Batch 3,040  of  5,625.    Elapsed: 0:31:01.\n","  Batch 3,080  of  5,625.    Elapsed: 0:31:26.\n","  Batch 3,120  of  5,625.    Elapsed: 0:31:50.\n","  Batch 3,160  of  5,625.    Elapsed: 0:32:15.\n","  Batch 3,200  of  5,625.    Elapsed: 0:32:39.\n","  Batch 3,240  of  5,625.    Elapsed: 0:33:03.\n","  Batch 3,280  of  5,625.    Elapsed: 0:33:28.\n","  Batch 3,320  of  5,625.    Elapsed: 0:33:52.\n","  Batch 3,360  of  5,625.    Elapsed: 0:34:17.\n","  Batch 3,400  of  5,625.    Elapsed: 0:34:41.\n","  Batch 3,440  of  5,625.    Elapsed: 0:35:06.\n","  Batch 3,480  of  5,625.    Elapsed: 0:35:30.\n","  Batch 3,520  of  5,625.    Elapsed: 0:35:55.\n","  Batch 3,560  of  5,625.    Elapsed: 0:36:19.\n","  Batch 3,600  of  5,625.    Elapsed: 0:36:44.\n","  Batch 3,640  of  5,625.    Elapsed: 0:37:08.\n","  Batch 3,680  of  5,625.    Elapsed: 0:37:33.\n","  Batch 3,720  of  5,625.    Elapsed: 0:37:57.\n","  Batch 3,760  of  5,625.    Elapsed: 0:38:21.\n","  Batch 3,800  of  5,625.    Elapsed: 0:38:46.\n","  Batch 3,840  of  5,625.    Elapsed: 0:39:10.\n","  Batch 3,880  of  5,625.    Elapsed: 0:39:35.\n","  Batch 3,920  of  5,625.    Elapsed: 0:39:59.\n","  Batch 3,960  of  5,625.    Elapsed: 0:40:24.\n","  Batch 4,000  of  5,625.    Elapsed: 0:40:48.\n","  Batch 4,040  of  5,625.    Elapsed: 0:41:13.\n","  Batch 4,080  of  5,625.    Elapsed: 0:41:37.\n","  Batch 4,120  of  5,625.    Elapsed: 0:42:02.\n","  Batch 4,160  of  5,625.    Elapsed: 0:42:26.\n","  Batch 4,200  of  5,625.    Elapsed: 0:42:51.\n","  Batch 4,240  of  5,625.    Elapsed: 0:43:15.\n","  Batch 4,280  of  5,625.    Elapsed: 0:43:40.\n","  Batch 4,320  of  5,625.    Elapsed: 0:44:04.\n","  Batch 4,360  of  5,625.    Elapsed: 0:44:28.\n","  Batch 4,400  of  5,625.    Elapsed: 0:44:53.\n","  Batch 4,440  of  5,625.    Elapsed: 0:45:17.\n","  Batch 4,480  of  5,625.    Elapsed: 0:45:42.\n","  Batch 4,520  of  5,625.    Elapsed: 0:46:06.\n","  Batch 4,560  of  5,625.    Elapsed: 0:46:31.\n","  Batch 4,600  of  5,625.    Elapsed: 0:46:55.\n","  Batch 4,640  of  5,625.    Elapsed: 0:47:20.\n","  Batch 4,680  of  5,625.    Elapsed: 0:47:44.\n","  Batch 4,720  of  5,625.    Elapsed: 0:48:09.\n","  Batch 4,760  of  5,625.    Elapsed: 0:48:33.\n","  Batch 4,800  of  5,625.    Elapsed: 0:48:58.\n","  Batch 4,840  of  5,625.    Elapsed: 0:49:22.\n","  Batch 4,880  of  5,625.    Elapsed: 0:49:47.\n","  Batch 4,920  of  5,625.    Elapsed: 0:50:11.\n","  Batch 4,960  of  5,625.    Elapsed: 0:50:36.\n","  Batch 5,000  of  5,625.    Elapsed: 0:51:00.\n","  Batch 5,040  of  5,625.    Elapsed: 0:51:24.\n","  Batch 5,080  of  5,625.    Elapsed: 0:51:49.\n","  Batch 5,120  of  5,625.    Elapsed: 0:52:13.\n","  Batch 5,160  of  5,625.    Elapsed: 0:52:38.\n","  Batch 5,200  of  5,625.    Elapsed: 0:53:02.\n","  Batch 5,240  of  5,625.    Elapsed: 0:53:27.\n","  Batch 5,280  of  5,625.    Elapsed: 0:53:51.\n","  Batch 5,320  of  5,625.    Elapsed: 0:54:16.\n","  Batch 5,360  of  5,625.    Elapsed: 0:54:40.\n","  Batch 5,400  of  5,625.    Elapsed: 0:55:05.\n","  Batch 5,440  of  5,625.    Elapsed: 0:55:29.\n","  Batch 5,480  of  5,625.    Elapsed: 0:55:54.\n","  Batch 5,520  of  5,625.    Elapsed: 0:56:18.\n","  Batch 5,560  of  5,625.    Elapsed: 0:56:43.\n","  Batch 5,600  of  5,625.    Elapsed: 0:57:07.\n","\n","  Average training loss: 0.16\n","  Training epcoh took: 0:57:22\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:02:14\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XMqy9fnq_zvN","colab_type":"text"},"source":["Try using Trainer class"]},{"cell_type":"code","metadata":{"id":"nUI9UG444hr6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597698827790,"user_tz":420,"elapsed":14609519,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["import dataclasses\n","import logging\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","\n","import numpy as np\n","\n","from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n","from transformers import GlueDataTrainingArguments as DataTrainingArguments\n","from transformers import (\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    glue_compute_metrics,\n","    glue_output_modes,\n","    glue_tasks_num_labels,\n","    set_seed,\n",")\n","\n","logging.basicConfig(level=logging.INFO)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXOVm1lx_ueB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597698827791,"user_tz":420,"elapsed":14609506,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n","    )"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"coBeNrYK_vAV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597698827792,"user_tz":420,"elapsed":14609495,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["model_args = ModelArguments(\n","    model_name_or_path=\"distilbert-base-cased\",\n",")\n","data_args = DataTrainingArguments(task_name=\"mnli\", data_dir=\"./glue_data/MNLI\")\n","training_args = TrainingArguments(\n","    output_dir=\"./models/model_name\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_gpu_train_batch_size=32,\n","    per_gpu_eval_batch_size=128,\n","    num_train_epochs=1,\n","    logging_steps=500,\n","    logging_first_step=True,\n","    save_steps=1000,\n","    evaluate_during_training=True,\n",")"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"ids01riEAPYy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597698827792,"user_tz":420,"elapsed":14609483,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":["def compute_metrics(p: EvalPrediction) -> Dict:\n","    preds = np.argmax(p.predictions, axis=1)\n","    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQz0jwnMAPuW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1597698827793,"user_tz":420,"elapsed":14609471,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}},"outputId":"bbe30728-fbd8-4e1d-a928-d6118d50adea"},"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    # eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n",")"],"execution_count":41,"outputs":[{"output_type":"stream","text":["INFO:transformers.training_args:PyTorch: setting up devices\n","INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wHSzKEZcWVPZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597698827793,"user_tz":420,"elapsed":14609459,"user":{"displayName":"Chris_De_Leon stemaway","photoUrl":"","userId":"13836727382765092362"}}},"source":[""],"execution_count":41,"outputs":[]}]}